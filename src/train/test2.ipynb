{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b4083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_probs shape: (4096,) dtype: float64 sum = 1.0\n",
      "Step 0: loss = 2.1227703898063854, grad_norm = 1.7811951913498185\n",
      "Step 1: loss = 1.979033796085783, grad_norm = 1.5440016006809383\n",
      "Step 2: loss = 1.8565502380918972, grad_norm = 1.3609381344967806\n",
      "Step 3: loss = 1.7506885748103371, grad_norm = 1.2145111212377147\n",
      "Step 4: loss = 1.6582696498328624, grad_norm = 1.0939795831292471\n",
      "Step 5: loss = 1.5769218954175648, grad_norm = 0.993272973357924\n",
      "Step 6: loss = 1.5045233119027128, grad_norm = 0.9090857998676676\n",
      "Step 7: loss = 1.4395832217923097, grad_norm = 0.8360875420097215\n",
      "Step 8: loss = 1.3811703869025727, grad_norm = 0.7684622240196167\n",
      "Step 9: loss = 1.3287643109448601, grad_norm = 0.7058891850439615\n",
      "Step 10: loss = 1.2817679117514598, grad_norm = 0.6488233305261631\n",
      "Step 11: loss = 1.239480044847387, grad_norm = 0.5970893753792966\n",
      "Step 12: loss = 1.20135421694948, grad_norm = 0.5492544551016841\n",
      "Step 13: loss = 1.1668802716202835, grad_norm = 0.5067863853323277\n",
      "Step 14: loss = 1.1356970089499097, grad_norm = 0.46470739421314045\n",
      "Step 15: loss = 1.1078486133069114, grad_norm = 0.42990591775641795\n",
      "Step 16: loss = 1.0816747952509675, grad_norm = 0.40837664836503534\n",
      "Step 17: loss = 1.0575986472993921, grad_norm = 0.3714051756898387\n",
      "Step 18: loss = 1.0362649218013984, grad_norm = 0.33819314955658286\n",
      "Step 19: loss = 1.0174207917714546, grad_norm = 0.30957064423558\n",
      "Step 20: loss = 1.0007615229088827, grad_norm = 0.28478726994830544\n",
      "Step 21: loss = 0.9859948486385248, grad_norm = 0.26341064459014935\n",
      "Step 22: loss = 0.9728467553718734, grad_norm = 0.2449903995275978\n",
      "Step 23: loss = 0.9610777112247806, grad_norm = 0.22907581456431894\n",
      "Step 24: loss = 0.9504924030369826, grad_norm = 0.21548835971578395\n",
      "Step 25: loss = 0.9409141379418062, grad_norm = 0.20416262807930516\n",
      "Step 26: loss = 0.9321578526349957, grad_norm = 0.1946527439136081\n",
      "Step 27: loss = 0.924046515344627, grad_norm = 0.18590033653094998\n",
      "Step 28: loss = 0.9164742005474081, grad_norm = 0.17716900961526033\n",
      "Step 29: loss = 0.9094132117711874, grad_norm = 0.1690358804645117\n",
      "Step 30: loss = 0.9028539753312629, grad_norm = 0.16178514413828576\n",
      "Step 31: loss = 0.8967904115825202, grad_norm = 0.1556114936406939\n",
      "Step 32: loss = 0.8912061163113502, grad_norm = 0.15065090073997728\n",
      "Step 33: loss = 0.8860516210445688, grad_norm = 0.14710006151219274\n",
      "Step 34: loss = 0.8812473083895763, grad_norm = 0.14513581606868217\n",
      "Step 35: loss = 0.8767489101430699, grad_norm = 0.1439999061602587\n",
      "Step 36: loss = 0.872499181027865, grad_norm = 0.1433192016582653\n",
      "Step 37: loss = 0.8684277702042356, grad_norm = 0.14279796563082892\n"
     ]
    }
   ],
   "source": [
    "## labraries for training\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\" \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"gpu\"\n",
    "\n",
    "import pennylane as qml\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.train.mmdagg_probs import mmdagg_prob\n",
    "from qcbm import QCBM\n",
    "\n",
    "\n",
    "## for mi ansatz\n",
    "def mutual_information_matrix(bits: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    bits : (N, n)  0/1 ndarray\n",
    "    return: (n, n)  float64 JAX array,  diag -> 0\n",
    "    \"\"\"\n",
    "    N, n = bits.shape\n",
    "    bits = bits.astype(jnp.int32)                  # 确保 0/1 -> 0/1 int\n",
    "\n",
    "    # --- 1. single qubit edge probs P(q_k = 1) ---------------------------------\n",
    "    pk1 = bits.mean(axis=0)                       # (n,)  float64\n",
    "    pk0 = 1.0 - pk1                               # (n,)\n",
    "\n",
    "    # --- 2. 2 qubits unite probs P(q_i = a, q_j = b) ------------------------\n",
    "    #    P11(i,j) = mean( bits[:,i] & bits[:,j] )\n",
    "    bT        = bits.T                            # (n, N)\n",
    "    P11       = (bT[:, None, :] & bT[None, :, :]).mean(axis=-1)  # (n, n)\n",
    "    \n",
    "    P10 = pk1[:, None] - P11                      # (n, n)\n",
    "    P01 = pk1[None, :] - P11\n",
    "    P00 = 1.0 - (P11 + P10 + P01)\n",
    "\n",
    "    # --- 3. mutal info I(i,j) = Σ_{a,b∈{0,1}} P_ab log( P_ab / (P_a·P_b) ) --\n",
    "    eps  = 1e-12\n",
    "    P_ab = jnp.stack([P00, P01, P10, P11], axis=0)       # (4, n, n)\n",
    "    logt = jnp.log( jnp.clip(P_ab, eps) )\n",
    "    pk0_col = pk0[:, None]          # (n,1)\n",
    "    pk1_col = pk1[:, None]          # (n,1)\n",
    "    logm = jnp.log( jnp.clip(\n",
    "        jnp.stack([pk0_col*pk0,\n",
    "                pk0_col*pk1,\n",
    "                pk1_col*pk0, \n",
    "                pk1_col*pk1],\n",
    "                axis=0), eps) )\n",
    "    Iij  = jnp.sum(P_ab * (logt - logm), axis=0)         # (n, n)\n",
    "\n",
    "    # --- 4. diag reset & return jnp.ndarray -------------------------------\n",
    "    Iij = Iij.at[jnp.diag_indices(n)].set(0.0)\n",
    "    return Iij\n",
    "\n",
    "# ------------  init target probs & model & params ------------\n",
    "from itertools import product\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data_2d/Qubits12/train.csv\")\n",
    "n_bits = 12\n",
    "bit_cols = [f\"q{i}\" for i in range(n_bits)]\n",
    "bitstrings = (\n",
    "    df[bit_cols]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "counts = bitstrings.value_counts().sort_index()\n",
    "all_bits = [\"\".join(seq) for seq in product(\"01\", repeat=n_bits)]\n",
    "probs_full = pd.Series(0.0, index=all_bits, dtype=float)   # float64\n",
    "probs_full.update(counts / counts.sum())                   # 归一化\n",
    "\n",
    "gpu = jax.devices(\"gpu\")[0]\n",
    "target_probs = jax.device_put(jnp.asarray(probs_full.values, dtype=jnp.float64), gpu)\n",
    "\n",
    "print(\"target_probs shape:\", target_probs.shape,\n",
    "      \"dtype:\", target_probs.dtype,\n",
    "      \"sum =\", float(target_probs.sum()))\n",
    "\n",
    "# ------------ parameter counts ------------\n",
    "# P = 222, n = 12, L = 10\n",
    "def count_params1(n_bits: int, L: int) -> int:\n",
    "    \"\"\"\n",
    "    return params requested for ansatz1\n",
    "    \"\"\"\n",
    "    assert L % 2 == 0, \"for ansatz1, L must be even number\"\n",
    "    return int((3 * L / 2 + 1) * n_bits - (L / 2))\n",
    "\n",
    "# R = 3, C = 4, PL = 45, L = 5, P = 225 \n",
    "def count_params2(R: int, C: int, L: int, periodic: bool = False) -> int:\n",
    "    n = R * C\n",
    "    per_layer = 2 * n + (R * (C - 1) + C * (R - 1))\n",
    "    if periodic:\n",
    "        per_layer += R + C\n",
    "    return per_layer * L\n",
    "\n",
    "# R = 3, C = 4, PL = 41, L = 5, P = 205\n",
    "def count_params3(R: int, C: int, L: int, add_dt: bool = False) -> int:\n",
    "    n = R * C\n",
    "    per_layer = 2*n + R*(C-1) + (R-1)*C + (1 if add_dt else 0)\n",
    "    return per_layer * L\n",
    "\n",
    "# n = 12, L = 5, keep_edges = 20, extras = 6, P = 226\n",
    "def count_params4(n: int, L: int, keep_edges: int, extras: int = 6) -> int:\n",
    "    return 2*L*n + L*keep_edges + extras  \n",
    "\n",
    "# ------------ ansatz factory ------------\n",
    "\n",
    "from src.circuits.ansatz1 import hardware_efficient_ansatz\n",
    "from src.circuits.ansatz2 import ising_structured_ansatz\n",
    "from src.circuits.ansatz3 import eh2d_ansatz\n",
    "from src.circuits.ansatz4 import mi_ansatz\n",
    "## Control # of Params around 100\n",
    "\n",
    "\n",
    "ansatz = ising_structured_ansatz\n",
    "n_qubits= 12\n",
    "mmd_fn = mmdagg_prob\n",
    "R = 3\n",
    "C = 4\n",
    "keep_edges = 20\n",
    "L1 = 10\n",
    "L2 = 5\n",
    "\n",
    "def ansatz_set(ansatz):\n",
    "    if ansatz == hardware_efficient_ansatz:\n",
    "        pc = count_params1(n_bits, L1)\n",
    "        L = L1\n",
    "        id = 1\n",
    "\n",
    "    if ansatz == ising_structured_ansatz:\n",
    "        pc = count_params2(R, C, L2, False)\n",
    "        L = L2\n",
    "        id = 2\n",
    "\n",
    "    if ansatz == eh2d_ansatz:\n",
    "        pc = count_params3(R, C, L2)\n",
    "        L = L2\n",
    "        id = 3\n",
    "\n",
    "    if ansatz == mi_ansatz:\n",
    "        pc = count_params4(n_qubits, L2, keep_edges)\n",
    "        L = L2\n",
    "        id = 4\n",
    "        bit_np = df[bit_cols].values\n",
    "        mi_mat = mutual_information_matrix(bit_np)\n",
    "        triu_i, triu_j = jnp.triu_indices(n_qubits, k=1)\n",
    "        mi_flat   = mi_mat[triu_i, triu_j]\n",
    "        top_idx   = jnp.argsort(-mi_flat)[:keep_edges]\n",
    "        mi_edges  = [(int(triu_i[k]), int(triu_j[k])) for k in top_idx]  # [(i,j),...]\n",
    "        def ansatz_mi(params, wires, *, L=None, **kw):\n",
    "            return mi_ansatz(\n",
    "                params, wires,\n",
    "                mi_edges = mi_edges,\n",
    "                L = L,\n",
    "                **kw\n",
    "            )\n",
    "        ansatz = ansatz_mi\n",
    "    return ansatz, L, pc, id\n",
    "ansatz, L, pc, id = ansatz_set(ansatz)\n",
    "\n",
    "model = QCBM(ansatz, n_bits, L, mmd_fn, target_probs)\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = jax.random.normal(key, shape=(pc,))\n",
    "\n",
    "\n",
    "# ------------ training set up ------------\n",
    "import optax\n",
    "import catalyst\n",
    "\n",
    "# opt = optax.adam(1e-2)\n",
    "\n",
    "lr_sched = optax.exponential_decay(  # 从 1e‑2 → 每 200 步 × 0.9\n",
    "    init_value=1e-2, transition_steps=200, decay_rate=0.9, staircase=True\n",
    ")\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adam(lr_sched, b2=0.999)\n",
    ")\n",
    "\n",
    "\n",
    "def update_step(i, params, opt_state, loss_log):\n",
    "    loss_val, grads = catalyst.value_and_grad(model.loss)(params)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    loss_log = loss_log.at[i].set(loss_val)\n",
    "    grad_norm = optax.global_norm(grads)\n",
    "    catalyst.debug.print(\"Step {i}: loss = {loss}, grad_norm = {g}\", i=i, loss=loss_val, g=grad_norm)\n",
    "\n",
    "    return (params, opt_state, loss_log)\n",
    "\n",
    "@qml.qjit     \n",
    "def optimization(params, n_steps: int = 1000):\n",
    "    opt_state = opt.init(params)\n",
    "    loss_log  = jnp.zeros(n_steps, dtype=params.dtype)\n",
    "    params, opt_state, loss_log = qml.for_loop(\n",
    "        0, n_steps, 1\n",
    "    )(update_step)(params, opt_state, loss_log)\n",
    "    return params, loss_log\n",
    "\n",
    "\n",
    "# ------------ training steps ------------\n",
    "\n",
    "trained_params, loss_hist = optimization(params)\n",
    "\n",
    "jax.block_until_ready(trained_params)      # wait until training ends\n",
    "\n",
    "# ───── save ─────\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "run_id = f\"result{id}\"\n",
    "\n",
    "\n",
    "out_dir = Path(\"../../data/results/Qubits12\")/run_id\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(out_dir / f\"params.npy\",  jax.device_get(trained_params))\n",
    "np.save(out_dir / f\"loss.npy\", jax.device_get(loss_hist))\n",
    "\n",
    "print(f\"model params have saved to: {out_dir / 'params.npy'}\")\n",
    "print(f\"Loss record has saved to: {out_dir / 'loss.npy'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
