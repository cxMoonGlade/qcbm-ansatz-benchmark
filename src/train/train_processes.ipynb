{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e249a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labraries for training\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\" \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import pennylane as qml\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.train.mmdagg_probs import mmdagg_prob\n",
    "from qcbm import QCBM\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64a3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for mi ansatz\n",
    "def mutual_information_matrix(bits: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    bits : (N, n)  0/1 ndarray\n",
    "    return: (n, n)  float64 JAX array,  diag -> 0\n",
    "    \"\"\"\n",
    "    N, n = bits.shape\n",
    "    bits = bits.astype(jnp.int32)                  # 确保 0/1 -> 0/1 int\n",
    "\n",
    "    # --- 1. single qubit edge probs P(q_k = 1) ---------------------------------\n",
    "    pk1 = bits.mean(axis=0)                       # (n,)  float64\n",
    "    pk0 = 1.0 - pk1                               # (n,)\n",
    "\n",
    "    # --- 2. 2 qubits unite probs P(q_i = a, q_j = b) ------------------------\n",
    "    #    P11(i,j) = mean( bits[:,i] & bits[:,j] )\n",
    "    bT        = bits.T                            # (n, N)\n",
    "    P11       = (bT[:, None, :] & bT[None, :, :]).mean(axis=-1)  # (n, n)\n",
    "    \n",
    "    P10 = pk1[:, None] - P11                      # (n, n)\n",
    "    P01 = pk1[None, :] - P11\n",
    "    P00 = 1.0 - (P11 + P10 + P01)\n",
    "\n",
    "    # --- 3. mutal info I(i,j) = Σ_{a,b∈{0,1}} P_ab log( P_ab / (P_a·P_b) ) --\n",
    "    eps  = 1e-12\n",
    "    P_ab = jnp.stack([P00, P01, P10, P11], axis=0)       # (4, n, n)\n",
    "    logt = jnp.log( jnp.clip(P_ab, eps) )\n",
    "    pk0_col = pk0[:, None]          # (n,1)\n",
    "    pk1_col = pk1[:, None]          # (n,1)\n",
    "    logm = jnp.log( jnp.clip(\n",
    "        jnp.stack([pk0_col*pk0,\n",
    "                pk0_col*pk1,\n",
    "                pk1_col*pk0, \n",
    "                pk1_col*pk1],\n",
    "                axis=0), eps) )\n",
    "    Iij  = jnp.sum(P_ab * (logt - logm), axis=0)         # (n, n)\n",
    "\n",
    "    # --- 4. diag reset & return jnp.ndarray -------------------------------\n",
    "    Iij = Iij.at[jnp.diag_indices(n)].set(0.0)\n",
    "    return Iij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c344b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_probs shape: (4096,) dtype: float64 sum = 1.0\n"
     ]
    }
   ],
   "source": [
    "# ------------  init target probs & model & params ------------\n",
    "from itertools import product\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data_2d/Qubits12/train.csv\")\n",
    "n_bits = 12\n",
    "bit_cols = [f\"q{i}\" for i in range(n_bits)]\n",
    "bitstrings = (\n",
    "    df[bit_cols]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "counts = bitstrings.value_counts().sort_index()\n",
    "all_bits = [\"\".join(seq) for seq in product(\"01\", repeat=n_bits)]\n",
    "probs_full = pd.Series(0.0, index=all_bits, dtype=float)   # float64\n",
    "probs_full.update(counts / counts.sum())                   # 归一化\n",
    "\n",
    "gpu = jax.devices(\"gpu\")[0]\n",
    "target_probs = jax.device_put(jnp.asarray(probs_full.values, dtype=jnp.float64), gpu)\n",
    "\n",
    "print(\"target_probs shape:\", target_probs.shape,\n",
    "      \"dtype:\", target_probs.dtype,\n",
    "      \"sum =\", float(target_probs.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87049174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ parameter counts ------------\n",
    "# P = 222, n = 12, L = 10\n",
    "def count_params1(n_bits: int, L: int) -> int:\n",
    "    \"\"\"\n",
    "    return params requested for ansatz1\n",
    "    \"\"\"\n",
    "    assert L % 2 == 0, \"for ansatz1, L must be even number\"\n",
    "    return int((3 * L / 2 + 1) * n_bits - (L / 2))\n",
    "\n",
    "# R = 3, C = 4, PL = 45, L = 5, P = 225 \n",
    "def count_params2(R: int, C: int, L: int, periodic: bool = False) -> int:\n",
    "    n = R * C\n",
    "    per_layer = 2 * n + (R * (C - 1) + C * (R - 1))\n",
    "    if periodic:\n",
    "        per_layer += R + C\n",
    "    return per_layer * L\n",
    "\n",
    "# R = 3, C = 4, PL = 41, L = 5, P = 205\n",
    "def count_params3(R: int, C: int, L: int, add_dt: bool = False) -> int:\n",
    "    n = R * C\n",
    "    per_layer = 2*n + R*(C-1) + (R-1)*C + (1 if add_dt else 0)\n",
    "    return per_layer * L\n",
    "\n",
    "# n = 12, L = 5, keep_edges = 20, extras = 6, P = 226\n",
    "def count_params4(n: int, L: int, keep_edges: int, extras: int = 6) -> int:\n",
    "    return 2*L*n + L*keep_edges + extras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f0cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ ansatz factory ------------\n",
    "\n",
    "from src.circuits.ansatz1 import hardware_efficient_ansatz\n",
    "from src.circuits.ansatz2 import ising_structured_ansatz\n",
    "from src.circuits.ansatz3 import eh2d_ansatz\n",
    "from src.circuits.ansatz4 import mi_ansatz\n",
    "## Control # of Params around 100\n",
    "\n",
    "\n",
    "ansatz = mi_ansatz\n",
    "n_qubits= 12\n",
    "mmd_fn = mmdagg_prob\n",
    "R = 3\n",
    "C = 4\n",
    "keep_edges = 20\n",
    "L1 = 10\n",
    "L2 = 5\n",
    "\n",
    "def ansatz_set(ansatz):\n",
    "    if ansatz == hardware_efficient_ansatz:\n",
    "        pc = count_params1(n_bits, L1)\n",
    "        L = L1\n",
    "        id = 1\n",
    "\n",
    "    if ansatz == ising_structured_ansatz:\n",
    "        pc = count_params2(R, C, L2, False)\n",
    "        L = L2\n",
    "        id = 2\n",
    "\n",
    "    if ansatz == eh2d_ansatz:\n",
    "        pc = count_params3(R, C, L2)\n",
    "        L = L2\n",
    "        id = 3\n",
    "\n",
    "    if ansatz == mi_ansatz:\n",
    "        pc = count_params4(n_qubits, L2, keep_edges)\n",
    "        L = L2\n",
    "        id = 4\n",
    "        bit_np = df[bit_cols].values\n",
    "        mi_mat = mutual_information_matrix(bit_np)\n",
    "        triu_i, triu_j = jnp.triu_indices(n_qubits, k=1)\n",
    "        mi_flat   = mi_mat[triu_i, triu_j]\n",
    "        top_idx   = jnp.argsort(-mi_flat)[:keep_edges]\n",
    "        mi_edges  = [(int(triu_i[k]), int(triu_j[k])) for k in top_idx]  # [(i,j),...]\n",
    "        def ansatz_mi(params, wires, *, L=None, **kw):\n",
    "            return mi_ansatz(\n",
    "                params, wires,\n",
    "                mi_edges = mi_edges,\n",
    "                L = L,\n",
    "                **kw\n",
    "            )\n",
    "        ansatz = ansatz_mi\n",
    "    return ansatz, L, pc, id\n",
    "ansatz, L, pc, id = ansatz_set(ansatz)\n",
    "\n",
    "model = QCBM(ansatz, n_bits, L, mmd_fn, target_probs)\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = jax.random.normal(key, shape=(pc,))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.796781817135032 dtype: float64 device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ------------ quick sanity check ------------\n",
    "key  = jax.random.PRNGKey(0)\n",
    "params = jax.random.normal(key, (pc,), dtype=jnp.float64)\n",
    "\n",
    "loss_val = model.loss(params)\n",
    "print(\"loss =\", loss_val, \"dtype:\", loss_val.dtype, \"device:\", loss_val.device)\n",
    "\n",
    "grads = jax.grad(model.loss)(params)\n",
    "print(\"grads shape\", grads.shape, \"dtype:\", grads.dtype, \"device:\", grads.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ training set up ------------\n",
    "import optax\n",
    "import catalyst\n",
    "\n",
    "# opt = optax.adam(1e-2)\n",
    "\n",
    "lr_sched = optax.exponential_decay(  # 从 1e‑2 → 每 200 步 × 0.9\n",
    "    init_value=1e-2, transition_steps=200, decay_rate=0.9, staircase=True\n",
    ")\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adam(lr_sched, b2=0.999)\n",
    ")\n",
    " \n",
    "def update_step(i, params, opt_state, loss_log):\n",
    "    loss_val, grads = catalyst.value_and_grad(model.loss)(params)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    loss_log = loss_log.at[i].set(loss_val)\n",
    "    catalyst.debug.print(\"Step {i}: loss = {loss}\", i=i, loss=loss_val)\n",
    "    return (params, opt_state, loss_log)\n",
    "\n",
    "@qml.qjit     \n",
    "def optimization(params, n_steps: int = 1000):\n",
    "    opt_state = opt.init(params)\n",
    "    loss_log  = jnp.zeros(n_steps, dtype=params.dtype)\n",
    "    params, opt_state, loss_log = qml.for_loop(\n",
    "        0, n_steps, 1\n",
    "    )(update_step)(params, opt_state, loss_log)\n",
    "    return params, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1.1846474008466559\n",
      "Step 1: loss = 1.1350018447780472\n",
      "Step 2: loss = 1.0881789259325891\n",
      "Step 3: loss = 1.0439661711870294\n",
      "Step 4: loss = 1.002139116667091\n",
      "Step 5: loss = 0.9624873081103933\n",
      "Step 6: loss = 0.9248462850538741\n",
      "Step 7: loss = 0.8891101954120765\n",
      "Step 8: loss = 0.8551884818518953\n",
      "Step 9: loss = 0.8227948314837366\n",
      "Step 10: loss = 0.7916422532436682\n",
      "Step 11: loss = 0.761984458118863\n",
      "Step 12: loss = 0.7338575755691426\n",
      "Step 13: loss = 0.7071999842299365\n",
      "Step 14: loss = 0.6819487379046323\n",
      "Step 15: loss = 0.658043187022778\n",
      "Step 16: loss = 0.6354230264414427\n",
      "Step 17: loss = 0.6140281366182525\n",
      "Step 18: loss = 0.593800530398626\n",
      "Step 19: loss = 0.5746867136016119\n",
      "Step 20: loss = 0.5566374661393972\n",
      "Step 21: loss = 0.5396053500822414\n",
      "Step 22: loss = 0.5235422928120255\n",
      "Step 23: loss = 0.5083984909724755\n",
      "Step 24: loss = 0.4941226472788293\n",
      "Step 25: loss = 0.48066294258110187\n",
      "Step 26: loss = 0.46796798971759646\n",
      "Step 27: loss = 0.45598740486164757\n",
      "Step 28: loss = 0.4446722178380421\n",
      "Step 29: loss = 0.4339755511297384\n",
      "Step 30: loss = 0.4238537281361319\n",
      "Step 31: loss = 0.41426751915051196\n",
      "Step 32: loss = 0.4051829436714325\n",
      "Step 33: loss = 0.3965711337956053\n",
      "Step 34: loss = 0.3884071718187999\n",
      "Step 35: loss = 0.38066826088596317\n",
      "Step 36: loss = 0.37333180783082526\n",
      "Step 37: loss = 0.3663739427896368\n",
      "Step 38: loss = 0.35976878108646715\n",
      "Step 39: loss = 0.3534884750175472\n",
      "Step 40: loss = 0.34750388659888093\n",
      "Step 41: loss = 0.34178558996255787\n",
      "Step 42: loss = 0.33630491433988063\n",
      "Step 43: loss = 0.3310348438237174\n",
      "Step 44: loss = 0.32595072254335733\n",
      "Step 45: loss = 0.3210307958514754\n",
      "Step 46: loss = 0.31625662351785977\n",
      "Step 47: loss = 0.3116133595461618\n",
      "Step 48: loss = 0.30708985431073815\n",
      "Step 49: loss = 0.3026785313725397\n",
      "Step 50: loss = 0.29837502848273584\n",
      "Step 51: loss = 0.2941776438792205\n",
      "Step 52: loss = 0.29008667088258205\n",
      "Step 53: loss = 0.28610371566544873\n",
      "Step 54: loss = 0.2822310747789092\n",
      "Step 55: loss = 0.27847121406530634\n",
      "Step 56: loss = 0.27482635749720663\n",
      "Step 57: loss = 0.27129817642633486\n",
      "Step 58: loss = 0.2678875684072208\n",
      "Step 59: loss = 0.2645945219767007\n",
      "Step 60: loss = 0.26141806789915006\n",
      "Step 61: loss = 0.2583563129884316\n",
      "Step 62: loss = 0.25540654337234503\n",
      "Step 63: loss = 0.2525653754243941\n",
      "Step 64: loss = 0.24982893235183434\n",
      "Step 65: loss = 0.24719303070228285\n",
      "Step 66: loss = 0.244653368119969\n",
      "Step 67: loss = 0.24220570529405053\n",
      "Step 68: loss = 0.23984602974543032\n",
      "Step 69: loss = 0.23757068142218268\n",
      "Step 70: loss = 0.23537641724361308\n",
      "Step 71: loss = 0.23326039851959435\n",
      "Step 72: loss = 0.23122010058509826\n",
      "Step 73: loss = 0.22925316203682347\n",
      "Step 74: loss = 0.22735720467402012\n",
      "Step 75: loss = 0.22552966098341032\n",
      "Step 76: loss = 0.2237676450295378\n",
      "Step 77: loss = 0.22206789884030315\n",
      "Step 78: loss = 0.22042684152238132\n",
      "Step 79: loss = 0.21884073779118451\n",
      "Step 80: loss = 0.21730597660762715\n",
      "Step 81: loss = 0.21581940600281074\n",
      "Step 82: loss = 0.21437862590165538\n",
      "Step 83: loss = 0.2129821342718353\n",
      "Step 84: loss = 0.211629273392369\n",
      "Step 85: loss = 0.2103200032611778\n",
      "Step 86: loss = 0.2090545832858244\n",
      "Step 87: loss = 0.20783324695414876\n",
      "Step 88: loss = 0.20665592766119226\n",
      "Step 89: loss = 0.20552206762095537\n",
      "Step 90: loss = 0.20443052626788583\n",
      "Step 91: loss = 0.2033795930469728\n",
      "Step 92: loss = 0.20236709464107924\n",
      "Step 93: loss = 0.20139056928519788\n",
      "Step 94: loss = 0.20044746749980805\n",
      "Step 95: loss = 0.19953533518643451\n",
      "Step 96: loss = 0.19865194247331272\n",
      "Step 97: loss = 0.19779533654191928\n",
      "Step 98: loss = 0.19696381441353095\n",
      "Step 99: loss = 0.1961558291533567\n",
      "Step 100: loss = 0.19536985801358922\n",
      "Step 101: loss = 0.19460427090697846\n",
      "Step 102: loss = 0.1938572384160949\n",
      "Step 103: loss = 0.19312670736816892\n",
      "Step 104: loss = 0.19241045005653037\n",
      "Step 105: loss = 0.19170616747940472\n",
      "Step 106: loss = 0.19101160821860952\n",
      "Step 107: loss = 0.19032466173033852\n",
      "Step 108: loss = 0.1896433993548357\n",
      "Step 109: loss = 0.18896606081233486\n",
      "Step 110: loss = 0.1882910063186686\n",
      "Step 111: loss = 0.1876166651575273\n",
      "Step 112: loss = 0.18694150808891027\n",
      "Step 113: loss = 0.18626405767849602\n",
      "Step 114: loss = 0.18558293493662165\n",
      "Step 115: loss = 0.18489692901722934\n",
      "Step 116: loss = 0.18420507195090177\n",
      "Step 117: loss = 0.18350670302703037\n",
      "Step 118: loss = 0.1828015112402196\n",
      "Step 119: loss = 0.18208955033133245\n",
      "Step 120: loss = 0.18137122418970852\n",
      "Step 121: loss = 0.1806472410776358\n",
      "Step 122: loss = 0.17991853749947764\n",
      "Step 123: loss = 0.1791861767066731\n",
      "Step 124: loss = 0.17845123380851494\n",
      "Step 125: loss = 0.17771468641708402\n",
      "Step 126: loss = 0.17697733219952516\n",
      "Step 127: loss = 0.1762397491983704\n",
      "Step 128: loss = 0.17550230191289534\n",
      "Step 129: loss = 0.17476518127444648\n",
      "Step 130: loss = 0.17402845723617708\n",
      "Step 131: loss = 0.1732921236070458\n",
      "Step 132: loss = 0.17255612497175885\n",
      "Step 133: loss = 0.1718203687163827\n",
      "Step 134: loss = 0.171084733808543\n",
      "Step 135: loss = 0.1703490880239028\n",
      "Step 136: loss = 0.16961331814769712\n",
      "Step 137: loss = 0.16887736825961958\n",
      "Step 138: loss = 0.16814127458051886\n",
      "Step 139: loss = 0.16740518417828823\n",
      "Step 140: loss = 0.16666934887716878\n",
      "Step 141: loss = 0.16593409297889705\n",
      "Step 142: loss = 0.16519976126934655\n",
      "Step 143: loss = 0.16446665976817618\n",
      "Step 144: loss = 0.163735003938408\n",
      "Step 145: loss = 0.16300488687044665\n",
      "Step 146: loss = 0.16227627394525934\n",
      "Step 147: loss = 0.16154902255381307\n",
      "Step 148: loss = 0.16082291809354077\n",
      "Step 149: loss = 0.16009771291058944\n",
      "Step 150: loss = 0.15937315437097124\n",
      "Step 151: loss = 0.15864899178997147\n",
      "Step 152: loss = 0.15792495831338685\n",
      "Step 153: loss = 0.15720073109693422\n",
      "Step 154: loss = 0.15647587926204443\n",
      "Step 155: loss = 0.15574981264767807\n",
      "Step 156: loss = 0.15502174475931094\n",
      "Step 157: loss = 0.15429068096884432\n",
      "Step 158: loss = 0.15355543919453207\n",
      "Step 159: loss = 0.15281470573599595\n",
      "Step 160: loss = 0.1520671237156306\n",
      "Step 161: loss = 0.15131140471528645\n",
      "Step 162: loss = 0.15054644526353328\n",
      "Step 163: loss = 0.14977142200206106\n",
      "Step 164: loss = 0.14898583998305034\n",
      "Step 165: loss = 0.14818952327292553\n",
      "Step 166: loss = 0.14738256067603805\n",
      "Step 167: loss = 0.14656523591161066\n",
      "Step 168: loss = 0.1457379684682042\n",
      "Step 169: loss = 0.14490127297461827\n",
      "Step 170: loss = 0.14405572775525724\n",
      "Step 171: loss = 0.14320193949033685\n",
      "Step 172: loss = 0.14234049943887117\n",
      "Step 173: loss = 0.14147193768536556\n",
      "Step 174: loss = 0.14059668664645827\n",
      "Step 175: loss = 0.1397150616835595\n",
      "Step 176: loss = 0.13882725915449765\n",
      "Step 177: loss = 0.1379333663422645\n",
      "Step 178: loss = 0.13703337686904393\n",
      "Step 179: loss = 0.13612720910371373\n",
      "Step 180: loss = 0.1352147306425222\n",
      "Step 181: loss = 0.13429579557766907\n",
      "Step 182: loss = 0.13337030054265545\n",
      "Step 183: loss = 0.13243825990028743\n",
      "Step 184: loss = 0.13149989141286697\n",
      "Step 185: loss = 0.1305556948968306\n",
      "Step 186: loss = 0.12960650276367292\n",
      "Step 187: loss = 0.12865348692675813\n",
      "Step 188: loss = 0.12769812047398033\n",
      "Step 189: loss = 0.12674210813342562\n",
      "Step 190: loss = 0.12578730858536752\n",
      "Step 191: loss = 0.12483567018607529\n",
      "Step 192: loss = 0.12388919231359397\n",
      "Step 193: loss = 0.12294991339626977\n",
      "Step 194: loss = 0.12201991864843414\n",
      "Step 195: loss = 0.12110135709241454\n",
      "Step 196: loss = 0.1201964572389086\n",
      "Step 197: loss = 0.11930753158622566\n",
      "Step 198: loss = 0.11843696050959328\n",
      "Step 199: loss = 0.11758714640045319\n",
      "Step 200: loss = 0.11676043057245344\n",
      "Step 201: loss = 0.1160377760208359\n",
      "Step 202: loss = 0.11533651292393973\n",
      "Step 203: loss = 0.1146575771074877\n",
      "Step 204: loss = 0.11400149424075727\n",
      "Step 205: loss = 0.11336830597198021\n",
      "Step 206: loss = 0.11275756313183699\n",
      "Step 207: loss = 0.11216842087899878\n",
      "Step 208: loss = 0.1115998522834551\n",
      "Step 209: loss = 0.11105094845719202\n",
      "Step 210: loss = 0.11052120399933905\n",
      "Step 211: loss = 0.11001064158167133\n",
      "Step 212: loss = 0.10951966495680493\n",
      "Step 213: loss = 0.10904864485271405\n",
      "Step 214: loss = 0.10859735690618694\n",
      "Step 215: loss = 0.10816441925029108\n",
      "Step 216: loss = 0.10774680516728026\n",
      "Step 217: loss = 0.10733938532076791\n",
      "Step 218: loss = 0.1069343722313728\n",
      "Step 219: loss = 0.1065206303723175\n",
      "Step 220: loss = 0.10608326660495147\n",
      "Step 221: loss = 0.10560478653980382\n",
      "Step 222: loss = 0.1050696203694405\n",
      "Step 223: loss = 0.10447188081812331\n",
      "Step 224: loss = 0.10382146328077371\n",
      "Step 225: loss = 0.10314227106406175\n",
      "Step 226: loss = 0.10246314046189861\n",
      "Step 227: loss = 0.10180816559451666\n",
      "Step 228: loss = 0.10119111110849259\n",
      "Step 229: loss = 0.10061422194051278\n",
      "Step 230: loss = 0.10007012515055143\n",
      "Step 231: loss = 0.09954557714564513\n",
      "Step 232: loss = 0.09902595491404018\n",
      "Step 233: loss = 0.09849945578522779\n",
      "Step 234: loss = 0.09796015850095802\n",
      "Step 235: loss = 0.09740931801127874\n",
      "Step 236: loss = 0.09685445244857001\n",
      "Step 237: loss = 0.09630633809774593\n",
      "Step 238: loss = 0.09577508487313152\n",
      "Step 239: loss = 0.09526703727450431\n",
      "Step 240: loss = 0.09478358065244745\n",
      "Step 241: loss = 0.09432174700508258\n",
      "Step 242: loss = 0.09387582548969815\n",
      "Step 243: loss = 0.0934391684609183\n",
      "Step 244: loss = 0.09300568676662271\n",
      "Step 245: loss = 0.09257082823083664\n",
      "Step 246: loss = 0.09213201946106138\n",
      "Step 247: loss = 0.0916886456957938\n",
      "Step 248: loss = 0.09124169321974816\n",
      "Step 249: loss = 0.09079320364766343\n",
      "Step 250: loss = 0.0903456874406147\n",
      "Step 251: loss = 0.08990161451718875\n",
      "Step 252: loss = 0.08946305214724294\n",
      "Step 253: loss = 0.08903146925789678\n",
      "Step 254: loss = 0.08860768464254834\n",
      "Step 255: loss = 0.08819191263201608\n",
      "Step 256: loss = 0.08778385728558304\n",
      "Step 257: loss = 0.08738282197900828\n",
      "Step 258: loss = 0.08698782364138612\n",
      "Step 259: loss = 0.08659771484167819\n",
      "Step 260: loss = 0.0862113150702707\n",
      "Step 261: loss = 0.08582754025149665\n",
      "Step 262: loss = 0.08544550888574895\n",
      "Step 263: loss = 0.08506460304808731\n",
      "Step 264: loss = 0.0846844731713889\n",
      "Step 265: loss = 0.08430499117107451\n",
      "Step 266: loss = 0.08392616984066152\n",
      "Step 267: loss = 0.0835480730046092\n",
      "Step 268: loss = 0.08317073975854104\n",
      "Step 269: loss = 0.08279413899857936\n",
      "Step 270: loss = 0.0824181601883724\n",
      "Step 271: loss = 0.08204263584564686\n",
      "Step 272: loss = 0.08166738307306408\n",
      "Step 273: loss = 0.08129224748027328\n",
      "Step 274: loss = 0.08091713389853712\n",
      "Step 275: loss = 0.0805420138698442\n",
      "Step 276: loss = 0.08016690814220807\n",
      "Step 277: loss = 0.07979185064925451\n",
      "Step 278: loss = 0.07941684610187337\n",
      "Step 279: loss = 0.07904183473491183\n",
      "Step 280: loss = 0.07866667475829164\n",
      "Step 281: loss = 0.07829114692601685\n",
      "Step 282: loss = 0.07791497855832974\n",
      "Step 283: loss = 0.07753787867372769\n",
      "Step 284: loss = 0.07715957329630467\n",
      "Step 285: loss = 0.07677983101363539\n",
      "Step 286: loss = 0.07639847275049169\n",
      "Step 287: loss = 0.07601536493323309\n",
      "Step 288: loss = 0.07563039999751915\n",
      "Step 289: loss = 0.07524347120582717\n",
      "Step 290: loss = 0.0748544494184151\n",
      "Step 291: loss = 0.07446316800073283\n",
      "Step 292: loss = 0.07406941920059446\n",
      "Step 293: loss = 0.07367296207737092\n",
      "Step 294: loss = 0.07327353934662736\n",
      "Step 295: loss = 0.07287089896882952\n",
      "Step 296: loss = 0.07246481619106863\n",
      "Step 297: loss = 0.07205511282959441\n",
      "Step 298: loss = 0.07164167229543827\n",
      "Step 299: loss = 0.07122445051608998\n",
      "Step 300: loss = 0.07080348390924779\n",
      "Step 301: loss = 0.07037889565455989\n",
      "Step 302: loss = 0.06995090081262555\n",
      "Step 303: loss = 0.06951980979161713\n",
      "Step 304: loss = 0.06908602879546238\n",
      "Step 305: loss = 0.0686500556208931\n",
      "Step 306: loss = 0.06821246912020076\n",
      "Step 307: loss = 0.06777391343982699\n",
      "Step 308: loss = 0.067335075087651\n",
      "Step 309: loss = 0.0668966568459995\n",
      "Step 310: loss = 0.06645934946872693\n",
      "Step 311: loss = 0.06602380384974844\n",
      "Step 312: loss = 0.06559060615864021\n",
      "Step 313: loss = 0.06516025827592167\n",
      "Step 314: loss = 0.06473316539384398\n",
      "Step 315: loss = 0.06430963193187658\n",
      "Step 316: loss = 0.06388986592365666\n",
      "Step 317: loss = 0.063473990900634\n",
      "Step 318: loss = 0.06306206326869294\n",
      "Step 319: loss = 0.06265409250595687\n",
      "Step 320: loss = 0.06225006137844445\n",
      "Step 321: loss = 0.061849943790016494\n",
      "Step 322: loss = 0.06145371869539966\n",
      "Step 323: loss = 0.06106137944083579\n",
      "Step 324: loss = 0.060672938687382036\n",
      "Step 325: loss = 0.060288429554312156\n",
      "Step 326: loss = 0.05990790378744471\n",
      "Step 327: loss = 0.05953142772851597\n",
      "Step 328: loss = 0.05915907679396046\n",
      "Step 329: loss = 0.05879092916471688\n",
      "Step 330: loss = 0.058427059438348486\n",
      "Step 331: loss = 0.05806753301376031\n",
      "Step 332: loss = 0.05771240187038709\n",
      "Step 333: loss = 0.05736170213578096\n",
      "Step 334: loss = 0.05701545347114621\n",
      "Step 335: loss = 0.05667365996634201\n",
      "Step 336: loss = 0.056336312023553846\n",
      "Step 337: loss = 0.05600338868791842\n",
      "Step 338: loss = 0.05567486001954558\n",
      "Step 339: loss = 0.055350689243580393\n",
      "Step 340: loss = 0.055030834509410514\n",
      "Step 341: loss = 0.05471525011635091\n",
      "Step 342: loss = 0.05440388705171712\n",
      "Step 343: loss = 0.0540966927074954\n",
      "Step 344: loss = 0.053793609734230516\n",
      "Step 345: loss = 0.05349457413592816\n",
      "Step 346: loss = 0.053199512843306145\n",
      "Step 347: loss = 0.05290834105916303\n",
      "Step 348: loss = 0.05262095962572029\n",
      "Step 349: loss = 0.05233725255126176\n",
      "Step 350: loss = 0.052057084715605405\n",
      "Step 351: loss = 0.05178029970505518\n",
      "Step 352: loss = 0.051506717722400966\n",
      "Step 353: loss = 0.051236133550949255\n",
      "Step 354: loss = 0.05096831458285519\n",
      "Step 355: loss = 0.05070299892548029\n",
      "Step 356: loss = 0.05043989357926192\n",
      "Step 357: loss = 0.050178672662125334\n",
      "Step 358: loss = 0.04991897566342441\n",
      "Step 359: loss = 0.049660405750089585\n",
      "Step 360: loss = 0.0494025282028072\n",
      "Step 361: loss = 0.04914486910544655\n",
      "Step 362: loss = 0.04888691442963827\n",
      "Step 363: loss = 0.04862810964826834\n",
      "Step 364: loss = 0.048367859989459805\n",
      "Step 365: loss = 0.048105531420408826\n",
      "Step 366: loss = 0.047840452434117144\n",
      "Step 367: loss = 0.04757191669875619\n",
      "Step 368: loss = 0.047299186615185354\n",
      "Step 369: loss = 0.04702149781615678\n",
      "Step 370: loss = 0.04673806464084738\n",
      "Step 371: loss = 0.04644808663406803\n",
      "Step 372: loss = 0.046150756341579516\n",
      "Step 373: loss = 0.045845268563864824\n",
      "Step 374: loss = 0.04553083140728887\n",
      "Step 375: loss = 0.04520667991443495\n",
      "Step 376: loss = 0.04487209281186801\n",
      "Step 377: loss = 0.04452641313974703\n",
      "Step 378: loss = 0.044169073552210364\n",
      "Step 379: loss = 0.043799627107490774\n",
      "Step 380: loss = 0.04341778444542065\n",
      "Step 381: loss = 0.04302345842629596\n",
      "Step 382: loss = 0.04261681759162762\n",
      "Step 383: loss = 0.042198350072176205\n",
      "Step 384: loss = 0.0417689393447625\n",
      "Step 385: loss = 0.0413299514662707\n",
      "Step 386: loss = 0.04088332827722333\n",
      "Step 387: loss = 0.04043167039731287\n",
      "Step 388: loss = 0.03997827681026035\n",
      "Step 389: loss = 0.039527088431914936\n",
      "Step 390: loss = 0.03908247369333916\n",
      "Step 391: loss = 0.0386488145580042\n",
      "Step 392: loss = 0.03822991668714656\n",
      "Step 393: loss = 0.03782836786684405\n",
      "Step 394: loss = 0.037445057317413065\n",
      "Step 395: loss = 0.03707907748414005\n",
      "Step 396: loss = 0.03672811696053648\n",
      "Step 397: loss = 0.036389247505077126\n",
      "Step 398: loss = 0.036059815811050014\n",
      "Step 399: loss = 0.03573809537684156\n",
      "Step 400: loss = 0.03542348130526582\n",
      "Step 401: loss = 0.03514641015111626\n",
      "Step 402: loss = 0.03487544149233329\n",
      "Step 403: loss = 0.034611186955178494\n",
      "Step 404: loss = 0.03435402928453278\n",
      "Step 405: loss = 0.03410404774107011\n",
      "Step 406: loss = 0.033861075463654876\n",
      "Step 407: loss = 0.033624814437567895\n",
      "Step 408: loss = 0.03339494147073168\n",
      "Step 409: loss = 0.03317117016006452\n",
      "Step 410: loss = 0.03295326690280903\n",
      "Step 411: loss = 0.03274103883070672\n",
      "Step 412: loss = 0.03253431476696977\n",
      "Step 413: loss = 0.032332932299436304\n",
      "Step 414: loss = 0.032136733351919855\n",
      "Step 415: loss = 0.031945563835528115\n",
      "Step 416: loss = 0.03175927224068216\n",
      "Step 417: loss = 0.03157770547707725\n",
      "Step 418: loss = 0.03140070420234622\n",
      "Step 419: loss = 0.03122810141323952\n",
      "Step 420: loss = 0.031059726526921093\n",
      "Step 421: loss = 0.030895413938595274\n",
      "Step 422: loss = 0.030735012320739795\n",
      "Step 423: loss = 0.03057839035676918\n",
      "Step 424: loss = 0.030425436381022318\n",
      "Step 425: loss = 0.030276052374970185\n",
      "Step 426: loss = 0.0301301452383824\n",
      "Step 427: loss = 0.029987618958778422\n",
      "Step 428: loss = 0.029848370086033443\n",
      "Step 429: loss = 0.02971228669234506\n",
      "Step 430: loss = 0.029579249123405465\n",
      "Step 431: loss = 0.02944913033815885\n",
      "Step 432: loss = 0.02932179463280834\n",
      "Step 433: loss = 0.029197095328689582\n",
      "Step 434: loss = 0.029074873331267723\n",
      "Step 435: loss = 0.028954958615986775\n",
      "Step 436: loss = 0.02883717558279041\n",
      "Step 437: loss = 0.028721351264054035\n",
      "Step 438: loss = 0.028607323869169914\n",
      "Step 439: loss = 0.02849494880892451\n",
      "Step 440: loss = 0.02838410030251572\n",
      "Step 441: loss = 0.028274668395079073\n",
      "Step 442: loss = 0.028166552825301734\n",
      "Step 443: loss = 0.028059655974437093\n",
      "Step 444: loss = 0.027953876910828433\n",
      "Step 445: loss = 0.027849107658327914\n",
      "Step 446: loss = 0.027745231850043472\n",
      "Step 447: loss = 0.027642125344934036\n",
      "Step 448: loss = 0.02753965829930475\n",
      "Step 449: loss = 0.027437698392958038\n",
      "Step 450: loss = 0.0273361151035262\n",
      "Step 451: loss = 0.02723478492637672\n",
      "Step 452: loss = 0.02713359729734275\n",
      "Step 453: loss = 0.027032460871669214\n",
      "Step 454: loss = 0.026931309883159848\n",
      "Step 455: loss = 0.026830110521897398\n",
      "Step 456: loss = 0.026728867436063106\n",
      "Step 457: loss = 0.026627630362453247\n",
      "Step 458: loss = 0.026526500431557288\n",
      "Step 459: loss = 0.02642563500635457\n",
      "Step 460: loss = 0.026325249312080794\n",
      "Step 461: loss = 0.02622561295166801\n",
      "Step 462: loss = 0.026127039902450546\n",
      "Step 463: loss = 0.026029871728477122\n",
      "Step 464: loss = 0.02593445523327406\n",
      "Step 465: loss = 0.025841117175446363\n",
      "Step 466: loss = 0.02575013953967713\n",
      "Step 467: loss = 0.025661738934879476\n",
      "Step 468: loss = 0.025576052971490775\n",
      "Step 469: loss = 0.025493135178912232\n",
      "Step 470: loss = 0.025412958523830544\n",
      "Step 471: loss = 0.025335426246024417\n",
      "Step 472: loss = 0.025260387804462263\n",
      "Step 473: loss = 0.02518765733529735\n",
      "Step 474: loss = 0.025117032131599593\n",
      "Step 475: loss = 0.02504830912862557\n",
      "Step 476: loss = 0.024981298043521574\n",
      "Step 477: loss = 0.024915830512371572\n",
      "Step 478: loss = 0.02485176517377649\n",
      "Step 479: loss = 0.024788989105422737\n",
      "Step 480: loss = 0.02472741631373793\n",
      "Step 481: loss = 0.024666984121081804\n",
      "Step 482: loss = 0.024607648316818182\n",
      "Step 483: loss = 0.024549377867352288\n",
      "Step 484: loss = 0.024492149843744718\n",
      "Step 485: loss = 0.02443594504933637\n",
      "Step 486: loss = 0.02438074464033516\n",
      "Step 487: loss = 0.02432652790527184\n",
      "Step 488: loss = 0.024273271132962782\n",
      "Step 489: loss = 0.02422094735913807\n",
      "Step 490: loss = 0.024169526877059196\n",
      "Step 491: loss = 0.024118978185225817\n",
      "Step 492: loss = 0.024069269087280305\n",
      "Step 493: loss = 0.02402036770296277\n",
      "Step 494: loss = 0.023972243210705617\n",
      "Step 495: loss = 0.023924866228981965\n",
      "Step 496: loss = 0.02387820883474612\n",
      "Step 497: loss = 0.02383224429375638\n",
      "Step 498: loss = 0.02378694662429097\n",
      "Step 499: loss = 0.023742290126594318\n",
      "Step 500: loss = 0.023698248989429908\n",
      "Step 501: loss = 0.023654797044351035\n",
      "Step 502: loss = 0.02361190769242255\n",
      "Step 503: loss = 0.02356955398957708\n",
      "Step 504: loss = 0.023527708852283155\n",
      "Step 505: loss = 0.02348634533547722\n",
      "Step 506: loss = 0.023445436936233133\n",
      "Step 507: loss = 0.023404957884425023\n",
      "Step 508: loss = 0.02336488339150512\n",
      "Step 509: loss = 0.023325189838109436\n",
      "Step 510: loss = 0.02328585488979236\n",
      "Step 511: loss = 0.023246857538025856\n",
      "Step 512: loss = 0.023208178070856445\n",
      "Step 513: loss = 0.023169797984215267\n",
      "Step 514: loss = 0.023131699850263372\n",
      "Step 515: loss = 0.023093867162636547\n",
      "Step 516: loss = 0.0230562841794407\n",
      "Step 517: loss = 0.023018935783131824\n",
      "Step 518: loss = 0.022981807372251046\n",
      "Step 519: loss = 0.02294488479406859\n",
      "Step 520: loss = 0.022908154320497177\n",
      "Step 521: loss = 0.022871602663355875\n",
      "Step 522: loss = 0.02283521701999597\n",
      "Step 523: loss = 0.022798985137266036\n",
      "Step 524: loss = 0.022762895380847744\n",
      "Step 525: loss = 0.022726936798159084\n",
      "Step 526: loss = 0.02269109916580138\n",
      "Step 527: loss = 0.022655373016402258\n",
      "Step 528: loss = 0.02261974964394565\n",
      "Step 529: loss = 0.022584221090569692\n",
      "Step 530: loss = 0.022548780120623238\n",
      "Step 531: loss = 0.02251342018910413\n",
      "Step 532: loss = 0.022478135411264928\n",
      "Step 533: loss = 0.02244292053852975\n",
      "Step 534: loss = 0.022407770943523697\n",
      "Step 535: loss = 0.022372682614688538\n",
      "Step 536: loss = 0.02233765215923677\n",
      "Step 537: loss = 0.02230267681226402\n",
      "Step 538: loss = 0.02226775444959115\n",
      "Step 539: loss = 0.022232883602003616\n",
      "Step 540: loss = 0.02219806346864507\n",
      "Step 541: loss = 0.022163293927338436\n",
      "Step 542: loss = 0.022128575539572287\n",
      "Step 543: loss = 0.02209390954805817\n",
      "Step 544: loss = 0.02205929786528102\n",
      "Step 545: loss = 0.0220247430523577\n",
      "Step 546: loss = 0.021990248288581815\n",
      "Step 547: loss = 0.02195581733300063\n",
      "Step 548: loss = 0.021921454479975556\n",
      "Step 549: loss = 0.021887164510816053\n",
      "Step 550: loss = 0.021852952643275185\n",
      "Step 551: loss = 0.021818824480178738\n",
      "Step 552: loss = 0.021784785957902748\n",
      "Step 553: loss = 0.02175084329499907\n",
      "Step 554: loss = 0.0217170029410667\n",
      "Step 555: loss = 0.021683271525933984\n",
      "Step 556: loss = 0.02164965580929722\n",
      "Step 557: loss = 0.021616162631075138\n",
      "Step 558: loss = 0.02158279886284172\n",
      "Step 559: loss = 0.021549571360776943\n",
      "Step 560: loss = 0.021516486920619714\n",
      "Step 561: loss = 0.021483552235146378\n",
      "Step 562: loss = 0.02145077385470595\n",
      "Step 563: loss = 0.021418158151314454\n",
      "Step 564: loss = 0.021385711286756752\n",
      "Step 565: loss = 0.021353439185035874\n",
      "Step 566: loss = 0.021321347509391804\n",
      "Step 567: loss = 0.02128944164397923\n",
      "Step 568: loss = 0.02125772668015544\n",
      "Step 569: loss = 0.02122620740723114\n",
      "Step 570: loss = 0.02119488830740281\n",
      "Step 571: loss = 0.02116377355449951\n",
      "Step 572: loss = 0.021132867016086493\n",
      "Step 573: loss = 0.021102172258366397\n",
      "Step 574: loss = 0.02107169255330715\n",
      "Step 575: loss = 0.021041430887401544\n",
      "Step 576: loss = 0.021011389971540284\n",
      "Step 577: loss = 0.020981572251577544\n",
      "Step 578: loss = 0.02095197991927012\n",
      "Step 579: loss = 0.020922614923383393\n",
      "Step 580: loss = 0.020893478980805864\n",
      "Step 581: loss = 0.020864573587555064\n",
      "Step 582: loss = 0.020835900029570906\n",
      "Step 583: loss = 0.02080745939319538\n",
      "Step 584: loss = 0.020779252575245953\n",
      "Step 585: loss = 0.020751280292602092\n",
      "Step 586: loss = 0.020723543091255552\n",
      "Step 587: loss = 0.02069604135479276\n",
      "Step 588: loss = 0.02066877531230504\n",
      "Step 589: loss = 0.02064174504574751\n",
      "Step 590: loss = 0.020614950496766676\n",
      "Step 591: loss = 0.020588391473041943\n",
      "Step 592: loss = 0.020562067654187683\n",
      "Step 593: loss = 0.020535978597256074\n",
      "Step 594: loss = 0.020510123741910975\n",
      "Step 595: loss = 0.020484502415312297\n",
      "Step 596: loss = 0.020459113836776236\n",
      "Step 597: loss = 0.020433957122251076\n",
      "Step 598: loss = 0.020409031288652967\n",
      "Step 599: loss = 0.020384335258102883\n",
      "Step 600: loss = 0.020359867862085542\n",
      "Step 601: loss = 0.020338045303985343\n",
      "Step 602: loss = 0.02031641206005536\n",
      "Step 603: loss = 0.02029496601354937\n",
      "Step 604: loss = 0.020273705127386046\n",
      "Step 605: loss = 0.020252627432343687\n",
      "Step 606: loss = 0.02023173101665791\n",
      "Step 607: loss = 0.020211014016963586\n",
      "Step 608: loss = 0.020190474610489034\n",
      "Step 609: loss = 0.020170111008405135\n",
      "Step 610: loss = 0.02014992145019493\n",
      "Step 611: loss = 0.02012990419893134\n",
      "Step 612: loss = 0.020110057537339597\n",
      "Step 613: loss = 0.020090379764526847\n",
      "Step 614: loss = 0.020070869193291305\n",
      "Step 615: loss = 0.020051524147940397\n",
      "Step 616: loss = 0.020032342962560224\n",
      "Step 617: loss = 0.020013323979701104\n",
      "Step 618: loss = 0.019994465549432322\n",
      "Step 619: loss = 0.01997576602872218\n",
      "Step 620: loss = 0.019957223781087077\n",
      "Step 621: loss = 0.01993883717646658\n",
      "Step 622: loss = 0.019920604591275398\n",
      "Step 623: loss = 0.019902524408591804\n",
      "Step 624: loss = 0.01988459501845576\n",
      "Step 625: loss = 0.019866814818244337\n",
      "Step 626: loss = 0.01984918221310233\n",
      "Step 627: loss = 0.01983169561641847\n",
      "Step 628: loss = 0.01981435345033741\n",
      "Step 629: loss = 0.019797154146305347\n",
      "Step 630: loss = 0.019780096145646252\n",
      "Step 631: loss = 0.01976317790016747\n",
      "Step 632: loss = 0.019746397872784194\n",
      "Step 633: loss = 0.019729754538164553\n",
      "Step 634: loss = 0.019713246383384262\n",
      "Step 635: loss = 0.019696871908586593\n",
      "Step 636: loss = 0.019680629627644382\n",
      "Step 637: loss = 0.019664518068812066\n",
      "Step 638: loss = 0.019648535775368025\n",
      "Step 639: loss = 0.019632681306236233\n",
      "Step 640: loss = 0.019616953236582725\n",
      "Step 641: loss = 0.019601350158389052\n",
      "Step 642: loss = 0.01958587068099153\n",
      "Step 643: loss = 0.019570513431593696\n",
      "Step 644: loss = 0.019555277055745057\n",
      "Step 645: loss = 0.01954016021778835\n",
      "Step 646: loss = 0.019525161601273405\n",
      "Step 647: loss = 0.01951027990934026\n",
      "Step 648: loss = 0.019495513865068162\n",
      "Step 649: loss = 0.019480862211793043\n",
      "Step 650: loss = 0.019466323713388696\n",
      "Step 651: loss = 0.01945189715451838\n",
      "Step 652: loss = 0.019437581340847165\n",
      "Step 653: loss = 0.01942337509922437\n",
      "Step 654: loss = 0.019409277277830782\n",
      "Step 655: loss = 0.019395286746296283\n",
      "Step 656: loss = 0.01938140239578614\n",
      "Step 657: loss = 0.019367623139060867\n",
      "Step 658: loss = 0.019353947910503523\n",
      "Step 659: loss = 0.01934037566612628\n",
      "Step 660: loss = 0.019326905383543647\n",
      "Step 661: loss = 0.019313536061923507\n",
      "Step 662: loss = 0.01930026672190545\n",
      "Step 663: loss = 0.019287096405495883\n",
      "Step 664: loss = 0.019274024175931936\n",
      "Step 665: loss = 0.01926104911751999\n",
      "Step 666: loss = 0.019248170335448116\n",
      "Step 667: loss = 0.01923538695557302\n",
      "Step 668: loss = 0.019222698124186196\n",
      "Step 669: loss = 0.019210103007754924\n",
      "Step 670: loss = 0.01919760079264606\n",
      "Step 671: loss = 0.01918519068482833\n",
      "Step 672: loss = 0.019172871909557\n",
      "Step 673: loss = 0.019160643711038056\n",
      "Step 674: loss = 0.019148505352074567\n",
      "Step 675: loss = 0.01913645611369541\n",
      "Step 676: loss = 0.01912449529476412\n",
      "Step 677: loss = 0.019112622211570503\n",
      "Step 678: loss = 0.019100836197408155\n",
      "Step 679: loss = 0.019089136602130442\n",
      "Step 680: loss = 0.01907752279169763\n",
      "Step 681: loss = 0.019065994147703137\n",
      "Step 682: loss = 0.019054550066894268\n",
      "Step 683: loss = 0.019043189960672755\n",
      "Step 684: loss = 0.019031913254590086\n",
      "Step 685: loss = 0.01902071938782928\n",
      "Step 686: loss = 0.01900960781267422\n",
      "Step 687: loss = 0.018998577993975387\n",
      "Step 688: loss = 0.01898762940860088\n",
      "Step 689: loss = 0.018976761544884355\n",
      "Step 690: loss = 0.018965973902064462\n",
      "Step 691: loss = 0.01895526598971905\n",
      "Step 692: loss = 0.018944637327195108\n",
      "Step 693: loss = 0.018934087443034986\n",
      "Step 694: loss = 0.018923615874402912\n",
      "Step 695: loss = 0.018913222166504744\n",
      "Step 696: loss = 0.01890290587201444\n",
      "Step 697: loss = 0.018892666550494843\n",
      "Step 698: loss = 0.018882503767823418\n",
      "Step 699: loss = 0.01887241709561958\n",
      "Step 700: loss = 0.018862406110673937\n",
      "Step 701: loss = 0.018852470394383633\n",
      "Step 702: loss = 0.018842609532191523\n",
      "Step 703: loss = 0.018832823113031923\n",
      "Step 704: loss = 0.018823110728784374\n",
      "Step 705: loss = 0.018813471973735757\n",
      "Step 706: loss = 0.018803906444050455\n",
      "Step 707: loss = 0.018794413737253747\n",
      "Step 708: loss = 0.018784993451724536\n",
      "Step 709: loss = 0.018775645186200694\n",
      "Step 710: loss = 0.018766368539298518\n",
      "Step 711: loss = 0.01875716310904621\n",
      "Step 712: loss = 0.018748028492430246\n",
      "Step 713: loss = 0.018738964284959272\n",
      "Step 714: loss = 0.018729970080243817\n",
      "Step 715: loss = 0.01872104546959212\n",
      "Step 716: loss = 0.018712190041623576\n",
      "Step 717: loss = 0.018703403381900817\n",
      "Step 718: loss = 0.018694685072580857\n",
      "Step 719: loss = 0.01868603469208446\n",
      "Step 720: loss = 0.018677451814786087\n",
      "Step 721: loss = 0.018668936010722566\n",
      "Step 722: loss = 0.01866048684532345\n",
      "Step 723: loss = 0.018652103879163072\n",
      "Step 724: loss = 0.01864378666772847\n",
      "Step 725: loss = 0.01863553476121491\n",
      "Step 726: loss = 0.018627347704337764\n",
      "Step 727: loss = 0.01861922503616749\n",
      "Step 728: loss = 0.018611166289985208\n",
      "Step 729: loss = 0.01860317099316031\n",
      "Step 730: loss = 0.018595238667047487\n",
      "Step 731: loss = 0.01858736882690755\n",
      "Step 732: loss = 0.018579560981844864\n",
      "Step 733: loss = 0.018571814634769777\n",
      "Step 734: loss = 0.01856412928237751\n",
      "Step 735: loss = 0.018556504415149676\n",
      "Step 736: loss = 0.018548939517371247\n",
      "Step 737: loss = 0.01854143406717253\n",
      "Step 738: loss = 0.018533987536583172\n",
      "Step 739: loss = 0.01852659939160761\n",
      "Step 740: loss = 0.018519269092316215\n",
      "Step 741: loss = 0.01851199609295559\n",
      "Step 742: loss = 0.018504779842070745\n",
      "Step 743: loss = 0.0184976197826476\n",
      "Step 744: loss = 0.018490515352265927\n",
      "Step 745: loss = 0.018483465983268773\n",
      "Step 746: loss = 0.01847647110294478\n",
      "Step 747: loss = 0.018469530133722097\n",
      "Step 748: loss = 0.018462642493375144\n",
      "Step 749: loss = 0.018455807595242325\n",
      "Step 750: loss = 0.018449024848453004\n",
      "Step 751: loss = 0.018442293658166044\n",
      "Step 752: loss = 0.018435613425814314\n",
      "Step 753: loss = 0.018428983549358104\n",
      "Step 754: loss = 0.018422403423548877\n",
      "Step 755: loss = 0.018415872440194207\n",
      "Step 756: loss = 0.01840938998843311\n",
      "Step 757: loss = 0.01840295545501246\n",
      "Step 758: loss = 0.018396568224571818\n",
      "Step 759: loss = 0.018390227679928182\n",
      "Step 760: loss = 0.0183839332023661\n",
      "Step 761: loss = 0.01837768417193049\n",
      "Step 762: loss = 0.018371479967717406\n",
      "Step 763: loss = 0.01836531996817061\n",
      "Step 764: loss = 0.018359203551378236\n",
      "Step 765: loss = 0.018353130095363417\n",
      "Step 766: loss = 0.018347098978384217\n",
      "Step 767: loss = 0.018341109579224742\n",
      "Step 768: loss = 0.01833516127748779\n",
      "Step 769: loss = 0.01832925345388687\n",
      "Step 770: loss = 0.018323385490533427\n",
      "Step 771: loss = 0.018317556771224826\n",
      "Step 772: loss = 0.018311766681724715\n",
      "Step 773: loss = 0.01830601461004705\n",
      "Step 774: loss = 0.0183002999467295\n",
      "Step 775: loss = 0.01829462208510917\n",
      "Step 776: loss = 0.018288980421589676\n",
      "Step 777: loss = 0.018283374355908724\n",
      "Step 778: loss = 0.01827780329139769\n",
      "Step 779: loss = 0.018272266635239526\n",
      "Step 780: loss = 0.018266763798721487\n",
      "Step 781: loss = 0.018261294197481442\n",
      "Step 782: loss = 0.018255857251754153\n",
      "Step 783: loss = 0.01825045238660845\n",
      "Step 784: loss = 0.018245079032181453\n",
      "Step 785: loss = 0.01823973662390921\n",
      "Step 786: loss = 0.018234424602750304\n",
      "Step 787: loss = 0.018229142415408458\n",
      "Step 788: loss = 0.018223889514545448\n",
      "Step 789: loss = 0.018218665358994905\n",
      "Step 790: loss = 0.0182134694139656\n",
      "Step 791: loss = 0.01820830115124637\n",
      "Step 792: loss = 0.01820316004940151\n",
      "Step 793: loss = 0.018198045593963017\n",
      "Step 794: loss = 0.018192957277620197\n",
      "Step 795: loss = 0.018187894600402023\n",
      "Step 796: loss = 0.018182857069855684\n",
      "Step 797: loss = 0.018177844201222086\n",
      "Step 798: loss = 0.018172855517605016\n",
      "Step 799: loss = 0.018167890550135606\n",
      "Step 800: loss = 0.01816294883813321\n",
      "Step 801: loss = 0.018158521209451953\n",
      "Step 802: loss = 0.018154112393370385\n",
      "Step 803: loss = 0.018149721934233202\n",
      "Step 804: loss = 0.018145349397054356\n",
      "Step 805: loss = 0.018140994366056935\n",
      "Step 806: loss = 0.018136656443363625\n",
      "Step 807: loss = 0.018132335247838513\n",
      "Step 808: loss = 0.018128030414049225\n",
      "Step 809: loss = 0.018123741591339063\n",
      "Step 810: loss = 0.018119468442999845\n",
      "Step 811: loss = 0.018115210645534403\n",
      "Step 812: loss = 0.01811096788800204\n",
      "Step 813: loss = 0.01810673987143849\n",
      "Step 814: loss = 0.01810252630833496\n",
      "Step 815: loss = 0.0180983269221696\n",
      "Step 816: loss = 0.01809414144698685\n",
      "Step 817: loss = 0.01808996962701506\n",
      "Step 818: loss = 0.01808581121632771\n",
      "Step 819: loss = 0.018081665978538736\n",
      "Step 820: loss = 0.01807753368653247\n",
      "Step 821: loss = 0.018073414122215234\n",
      "Step 822: loss = 0.01806930707629985\n",
      "Step 823: loss = 0.018065212348102638\n",
      "Step 824: loss = 0.018061129745366566\n",
      "Step 825: loss = 0.018057059084101573\n",
      "Step 826: loss = 0.018053000188436755\n",
      "Step 827: loss = 0.018048952890485335\n",
      "Step 828: loss = 0.018044917030219938\n",
      "Step 829: loss = 0.01804089245535233\n",
      "Step 830: loss = 0.018036879021218796\n",
      "Step 831: loss = 0.018032876590676265\n",
      "Step 832: loss = 0.01802888503399895\n",
      "Step 833: loss = 0.018024904228784715\n",
      "Step 834: loss = 0.018020934059859512\n",
      "Step 835: loss = 0.01801697441919071\n",
      "Step 836: loss = 0.018013025205798134\n",
      "Step 837: loss = 0.018009086325666487\n",
      "Step 838: loss = 0.018005157691658676\n",
      "Step 839: loss = 0.018001239223425943\n",
      "Step 840: loss = 0.017997330847319682\n",
      "Step 841: loss = 0.017993432496293742\n",
      "Step 842: loss = 0.01798954410981028\n",
      "Step 843: loss = 0.017985665633740325\n",
      "Step 844: loss = 0.01798179702026078\n",
      "Step 845: loss = 0.01797793822774985\n",
      "Step 846: loss = 0.01797408922068254\n",
      "Step 847: loss = 0.01797024996951942\n",
      "Step 848: loss = 0.017966420450597077\n",
      "Step 849: loss = 0.017962600646012898\n",
      "Step 850: loss = 0.01795879054350851\n",
      "Step 851: loss = 0.017954990136352073\n",
      "Step 852: loss = 0.017951199423212796\n",
      "Step 853: loss = 0.017947418408036528\n",
      "Step 854: loss = 0.017943647099916044\n",
      "Step 855: loss = 0.017939885512956267\n",
      "Step 856: loss = 0.01793613366613853\n",
      "Step 857: loss = 0.017932391583181637\n",
      "Step 858: loss = 0.017928659292394702\n",
      "Step 859: loss = 0.017924936826536363\n",
      "Step 860: loss = 0.017921224222662596\n",
      "Step 861: loss = 0.017917521521976582\n",
      "Step 862: loss = 0.01791382876967528\n",
      "Step 863: loss = 0.017910146014793914\n",
      "Step 864: loss = 0.01790647331004572\n",
      "Step 865: loss = 0.017902810711664826\n",
      "Step 866: loss = 0.01789915827923999\n",
      "Step 867: loss = 0.017895516075552585\n",
      "Step 868: loss = 0.017891884166410355\n",
      "Step 869: loss = 0.017888262620477068\n",
      "Step 870: loss = 0.017884651509104402\n",
      "Step 871: loss = 0.017881050906160646\n",
      "Step 872: loss = 0.017877460887859138\n",
      "Step 873: loss = 0.017873881532584047\n",
      "Step 874: loss = 0.017870312920717255\n",
      "Step 875: loss = 0.01786675513446447\n",
      "Step 876: loss = 0.01786320825767782\n",
      "Step 877: loss = 0.017859672375682013\n",
      "Step 878: loss = 0.017856147575099915\n",
      "Step 879: loss = 0.017852633943672796\n",
      "Step 880: loss = 0.017849131570089136\n",
      "Step 881: loss = 0.01784564054380667\n",
      "Step 882: loss = 0.017842160954878672\n",
      "Step 883: loss = 0.01783869289378105\n",
      "Step 884: loss = 0.017835236451239436\n",
      "Step 885: loss = 0.01783179171805798\n",
      "Step 886: loss = 0.017828358784950305\n",
      "Step 887: loss = 0.017824937742372697\n",
      "Step 888: loss = 0.01782152868035678\n",
      "Step 889: loss = 0.017818131688347092\n",
      "Step 890: loss = 0.01781474685504175\n",
      "Step 891: loss = 0.017811374268229967\n",
      "Step 892: loss = 0.017808014014639335\n",
      "Step 893: loss = 0.017804666179782092\n",
      "Step 894: loss = 0.017801330847804726\n",
      "Step 895: loss = 0.017798008101342754\n",
      "Step 896: loss = 0.017794698021375344\n",
      "Step 897: loss = 0.017791400687088482\n",
      "Step 898: loss = 0.017788116175737508\n",
      "Step 899: loss = 0.017784844562516843\n",
      "Step 900: loss = 0.017781585920432223\n",
      "Step 901: loss = 0.01777834032017672\n",
      "Step 902: loss = 0.017775107830013896\n",
      "Step 903: loss = 0.017771888515661135\n",
      "Step 904: loss = 0.017768682440182596\n",
      "Step 905: loss = 0.01776548966388161\n",
      "Step 906: loss = 0.017762310244202776\n",
      "Step 907: loss = 0.0177591442356352\n",
      "Step 908: loss = 0.017755991689621702\n",
      "Step 909: loss = 0.017752852654473465\n",
      "Step 910: loss = 0.017749727175291028\n",
      "Step 911: loss = 0.017746615293884223\n",
      "Step 912: loss = 0.017743517048706203\n",
      "Step 913: loss = 0.017740432474784892\n",
      "Step 914: loss = 0.017737361603661895\n",
      "Step 915: loss = 0.017734304463338974\n",
      "Step 916: loss = 0.017731261078224245\n",
      "Step 917: loss = 0.017728231469088553\n",
      "Step 918: loss = 0.01772521565302294\n",
      "Step 919: loss = 0.017722213643402467\n",
      "Step 920: loss = 0.0177192254498551\n",
      "Step 921: loss = 0.01771625107823461\n",
      "Step 922: loss = 0.017713290530597968\n",
      "Step 923: loss = 0.017710343805186945\n",
      "Step 924: loss = 0.017707410896416826\n",
      "Step 925: loss = 0.017704491794865433\n",
      "Step 926: loss = 0.017701586487268606\n",
      "Step 927: loss = 0.017698694956520246\n",
      "Step 928: loss = 0.017695817181674848\n",
      "Step 929: loss = 0.017692953137955405\n",
      "Step 930: loss = 0.01769010279676313\n",
      "Step 931: loss = 0.01768726612569305\n",
      "Step 932: loss = 0.017684443088551972\n",
      "Step 933: loss = 0.017681633645378236\n",
      "Step 934: loss = 0.01767883775246939\n",
      "Step 935: loss = 0.017676055362407142\n",
      "Step 936: loss = 0.0176732864240886\n",
      "Step 937: loss = 0.017670530882760268\n",
      "Step 938: loss = 0.017667788680054496\n",
      "Step 939: loss = 0.017665059754026602\n",
      "Step 940: loss = 0.01766234403919772\n",
      "Step 941: loss = 0.017659641466595422\n",
      "Step 942: loss = 0.01765695196380258\n",
      "Step 943: loss = 0.017654275455000413\n",
      "Step 944: loss = 0.017651611861021703\n",
      "Step 945: loss = 0.01764896109939714\n",
      "Step 946: loss = 0.017646323084410755\n",
      "Step 947: loss = 0.017643697727151443\n",
      "Step 948: loss = 0.017641084935568343\n",
      "Step 949: loss = 0.017638484614525777\n",
      "Step 950: loss = 0.017635896665861177\n",
      "Step 951: loss = 0.0176333209884407\n",
      "Step 952: loss = 0.01763075747821875\n",
      "Step 953: loss = 0.017628206028294794\n",
      "Step 954: loss = 0.017625666528975455\n",
      "Step 955: loss = 0.017623138867829447\n",
      "Step 956: loss = 0.017620622929750324\n",
      "Step 957: loss = 0.017618118597014936\n",
      "Step 958: loss = 0.017615625749340825\n",
      "Step 959: loss = 0.017613144263948565\n",
      "Step 960: loss = 0.01761067401561785\n",
      "Step 961: loss = 0.01760821487674754\n",
      "Step 962: loss = 0.01760576671741162\n",
      "Step 963: loss = 0.017603329405418595\n",
      "Step 964: loss = 0.01760090280636643\n",
      "Step 965: loss = 0.017598486783698644\n",
      "Step 966: loss = 0.01759608119875931\n",
      "Step 967: loss = 0.017593685910846705\n",
      "Step 968: loss = 0.01759130077726508\n",
      "Step 969: loss = 0.017588925653377947\n",
      "Step 970: loss = 0.017586560392657817\n",
      "Step 971: loss = 0.01758420484673371\n",
      "Step 972: loss = 0.017581858865442307\n",
      "Step 973: loss = 0.017579522296870043\n",
      "Step 974: loss = 0.017577194987401214\n",
      "Step 975: loss = 0.0175748767817611\n",
      "Step 976: loss = 0.017572567523055083\n",
      "Step 977: loss = 0.017570267052811544\n",
      "Step 978: loss = 0.017567975211019594\n",
      "Step 979: loss = 0.017565691836165603\n",
      "Step 980: loss = 0.017563416765268323\n",
      "Step 981: loss = 0.01756114983391147\n",
      "Step 982: loss = 0.017558890876276097\n",
      "Step 983: loss = 0.017556639725170926\n",
      "Step 984: loss = 0.017554396212057767\n",
      "Step 985: loss = 0.017552160167079906\n",
      "Step 986: loss = 0.017549931419084255\n",
      "Step 987: loss = 0.017547709795644205\n",
      "Step 988: loss = 0.017545495123080145\n",
      "Step 989: loss = 0.017543287226476263\n",
      "Step 990: loss = 0.01754108592969914\n",
      "Step 991: loss = 0.017538891055408908\n",
      "Step 992: loss = 0.017536702425074466\n",
      "Step 993: loss = 0.017534519858981853\n",
      "Step 994: loss = 0.017532343176244827\n",
      "Step 995: loss = 0.017530172194807712\n",
      "Step 996: loss = 0.017528006731454677\n",
      "Step 997: loss = 0.017525846601807758\n",
      "Step 998: loss = 0.01752369162033041\n",
      "Step 999: loss = 0.0175215416003253\n"
     ]
    }
   ],
   "source": [
    "# ------------ training steps ------------\n",
    "trained_params, loss_hist = optimization(params, n_steps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee1565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数已保存至: ../../data/results/result4/params.npy\n",
      "Loss 记录已保存至: ../../data/results/result4/loss.npy\n"
     ]
    }
   ],
   "source": [
    "jax.block_until_ready(trained_params)      # wait until training ends\n",
    "\n",
    "# ───── save ─────\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "run_id = f\"result{id}\"\n",
    "\n",
    "\n",
    "out_dir = Path(\"../../data/results\")/run_id\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(out_dir / f\"params.npy\",  jax.device_get(trained_params))\n",
    "np.save(out_dir / f\"loss.npy\", jax.device_get(loss_hist))\n",
    "\n",
    "print(f\"model params have saved to: {out_dir / 'params.npy'}\")\n",
    "print(f\"Loss record has saved to: {out_dir / 'loss.npy'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba039fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after reload: 0.017519396353931176\n",
      "KL   : 5.8318e-02\n",
      "KL (target>0 only): 0.023321320713117023\n",
      "L1   : 4.7178e-04\n",
      "MMD  : 3.4205e-05\n"
     ]
    }
   ],
   "source": [
    "# ───── 2. load the model ─────\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "out_dir = Path(f\"../../data/results/result{id}\")\n",
    "loaded_params = jnp.asarray(np.load(out_dir/\"params.npy\"), dtype=jnp.float64)\n",
    "\n",
    "model = QCBM(ansatz, n_bits, L, mmdagg_prob, target_probs)  # 跟训练时相同\n",
    "loss_loaded = model.loss(loaded_params).block_until_ready()\n",
    "print(\"Loss after reload:\", float(loss_loaded))\n",
    "\n",
    "probs_trained = model.circuit(loaded_params).block_until_ready()\n",
    "kl   = scipy.stats.entropy(target_probs, probs_trained)\n",
    "l1   = jnp.mean(jnp.abs(target_probs - probs_trained))\n",
    "gpu = jax.devices(\"gpu\")[0]\n",
    "mmd, _ = mmdagg_prob(jax.device_put(target_probs, gpu), \n",
    "                     jax.device_put(probs_trained, gpu),\n",
    "                     kernel=\"laplace_gaussian\", number_bandwidths=10)\n",
    "mask = target_probs > 0          # only check the non-zero part\n",
    "kl_nonzero = scipy.stats.entropy(\n",
    "    target_probs[mask],\n",
    "    probs_trained[mask] / probs_trained[mask].sum()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"KL   : {kl:.4e}\")\n",
    "print(\"KL (target>0 only):\", kl_nonzero)\n",
    "print(f\"L1   : {l1:.4e}\")\n",
    "print(f\"MMD  : {mmd:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041dfe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 losses: [1.1846474  1.13500184 1.08817893 1.04396617 1.00213912]\n",
      "Final   loss : 0.0175215416003253\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params_loaded   = loaded_params\n",
    "loss_hist_loaded = np.load(f\"{out_dir}/loss.npy\")\n",
    "\n",
    "print(\"First 5 losses:\", loss_hist_loaded[:5])\n",
    "print(\"Final   loss :\", loss_hist_loaded[-1])\n",
    "\n",
    "plt.semilogy(loss_hist_loaded)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss (log‑scale)\")\n",
    "plt.title(\"Training curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e54516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST  :  KL = 1.4397e+00   L1 = 5.1414e-03   MMD = 2.4332e-02\n",
      "UNSEEN:  KL = 1.5082e+00   L1 = 5.3655e-03   MMD = 2.3912e-02\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR   = \"../../data\"\n",
    "def csv_to_probs(path: str, n_bits=8, dtype=jnp.float64):\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    bitstrings = df[\"bitstring\"].str.strip().str.zfill(n_bits)\n",
    "    counts = bitstrings.value_counts().sort_index()\n",
    "    all_bits = [\"\".join(seq) for seq in product(\"01\", repeat=n_bits)]\n",
    "    probs = pd.Series(0.0, index=all_bits, dtype=float)\n",
    "    probs.update(counts / counts.sum())\n",
    "    return jnp.asarray(probs.values, dtype=dtype)\n",
    "\n",
    "def kl_div(p, q, eps=1e-10):\n",
    "    p = jnp.clip(p, eps, 1.0)\n",
    "    q = jnp.clip(q, eps, 1.0)\n",
    "    p = p / p.sum();  q = q / q.sum()\n",
    "    return jnp.sum(p * jnp.log(p / q))\n",
    "\n",
    "# ---------- load trained params ----------\n",
    "params_loaded = params_loaded\n",
    "\n",
    "# ---------- build QCBM ----------\n",
    "model = QCBM(\n",
    "    ansatz       = ansatz,\n",
    "    n_qubits     = 8,\n",
    "    L            = 4,\n",
    "    mmd_fn       = mmdagg_prob,\n",
    "    target_probs = jnp.zeros(256),   # 占位即可\n",
    "    dtype        = jnp.float64,\n",
    ")\n",
    "\n",
    "# ---------- K1, L1, MMD ----------\n",
    "@jax.jit\n",
    "def three_metrics(target_probs, params):\n",
    "    probs = model.circuit(params)\n",
    "    kl    = kl_div(target_probs, probs)\n",
    "    l1    = jnp.mean(jnp.abs(target_probs - probs))\n",
    "    mmd   = mmdagg_prob(\n",
    "        target_probs, probs,\n",
    "        kernel=\"laplace_gaussian\", number_bandwidths=10,\n",
    "        build_details=False, dtype=jnp.float64\n",
    "    )\n",
    "    return kl, l1, mmd\n",
    "\n",
    "# ---------- calculate & print ----------\n",
    "splits = {\n",
    "    \"TEST\"  : csv_to_probs(\"../../data_2d/test.csv\"),\n",
    "    \"UNSEEN\": csv_to_probs(\"../../data_2d/unseen.csv\"),\n",
    "}\n",
    "\n",
    "for name, tgt in splits.items():\n",
    "    kl, l1, mmd = three_metrics(tgt, params_loaded)\n",
    "    kl, l1, mmd = map(lambda x: float(x.block_until_ready()), (kl, l1, mmd))\n",
    "    print(f\"{name:6s}:  KL = {kl:.4e}   L1 = {l1:.4e}   MMD = {mmd:.4e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
