{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e249a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labraries for training\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\" \n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"gpu\"\n",
    "\n",
    "import pennylane as qml\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.train.mmdagg_probs import mmdagg_prob\n",
    "from qcbm import QCBM\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64a3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for mi ansatz\n",
    "def mutual_information_matrix(bits: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    bits : (N, n)  0/1 ndarray\n",
    "    return: (n, n)  float64 JAX array,  diag -> 0\n",
    "    \"\"\"\n",
    "    N, n = bits.shape\n",
    "    bits = bits.astype(jnp.int32)                  # 确保 0/1 -> 0/1 int\n",
    "\n",
    "    # --- 1. single qubit edge probs P(q_k = 1) ---------------------------------\n",
    "    pk1 = bits.mean(axis=0)                       # (n,)  float64\n",
    "    pk0 = 1.0 - pk1                               # (n,)\n",
    "\n",
    "    # --- 2. 2 qubits unite probs P(q_i = a, q_j = b) ------------------------\n",
    "    #    P11(i,j) = mean( bits[:,i] & bits[:,j] )\n",
    "    bT        = bits.T                            # (n, N)\n",
    "    P11       = (bT[:, None, :] & bT[None, :, :]).mean(axis=-1)  # (n, n)\n",
    "    \n",
    "    P10 = pk1[:, None] - P11                      # (n, n)\n",
    "    P01 = pk1[None, :] - P11\n",
    "    P00 = 1.0 - (P11 + P10 + P01)\n",
    "\n",
    "    # --- 3. mutal info I(i,j) = Σ_{a,b∈{0,1}} P_ab log( P_ab / (P_a·P_b) ) --\n",
    "    eps  = 1e-12\n",
    "    P_ab = jnp.stack([P00, P01, P10, P11], axis=0)       # (4, n, n)\n",
    "    logt = jnp.log( jnp.clip(P_ab, eps) )\n",
    "    pk0_col = pk0[:, None]          # (n,1)\n",
    "    pk1_col = pk1[:, None]          # (n,1)\n",
    "    logm = jnp.log( jnp.clip(\n",
    "        jnp.stack([pk0_col*pk0,\n",
    "                pk0_col*pk1,\n",
    "                pk1_col*pk0, \n",
    "                pk1_col*pk1],\n",
    "                axis=0), eps) )\n",
    "    Iij  = jnp.sum(P_ab * (logt - logm), axis=0)         # (n, n)\n",
    "\n",
    "    # --- 4. diag reset & return jnp.ndarray -------------------------------\n",
    "    Iij = Iij.at[jnp.diag_indices(n)].set(0.0)\n",
    "    return Iij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c344b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_probs shape: (4096,) dtype: float64 sum = 1.0\n"
     ]
    }
   ],
   "source": [
    "# ------------  init target probs & model & params ------------\n",
    "from itertools import product\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data_2d/Qubits12/train.csv\")\n",
    "n_bits = 12\n",
    "bit_cols = [f\"q{i}\" for i in range(n_bits)]\n",
    "bitstrings = (\n",
    "    df[bit_cols]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "counts = bitstrings.value_counts().sort_index()\n",
    "all_bits = [\"\".join(seq) for seq in product(\"01\", repeat=n_bits)]\n",
    "probs_full = pd.Series(0.0, index=all_bits, dtype=float)   # float64\n",
    "probs_full.update(counts / counts.sum())                   # 归一化\n",
    "\n",
    "gpu = jax.devices(\"gpu\")[0]\n",
    "target_probs = jax.device_put(jnp.asarray(probs_full.values, dtype=jnp.float64), gpu)\n",
    "\n",
    "print(\"target_probs shape:\", target_probs.shape,\n",
    "      \"dtype:\", target_probs.dtype,\n",
    "      \"sum =\", float(target_probs.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87049174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ parameter counts ------------\n",
    "# P = 222, n = 12, L = 10\n",
    "def count_params1(n_bits: int, L: int) -> int:\n",
    "    \"\"\"\n",
    "    return params requested for ansatz1\n",
    "    \"\"\"\n",
    "    assert L % 2 == 0, \"for ansatz1, L must be even number\"\n",
    "    return int((3 * L / 2 + 1) * n_bits - (L / 2))\n",
    "\n",
    "# R = 3, C = 4, PL = 45, L = 5, P = 225 \n",
    "def count_params2(R: int, C: int, L: int, periodic: bool = False) -> int:\n",
    "    n = R * C\n",
    "    per_layer = 2 * n + (R * (C - 1) + C * (R - 1))\n",
    "    if periodic:\n",
    "        per_layer += R + C\n",
    "    return per_layer * L\n",
    "\n",
    "# R = 3, C = 4, PL = 41, L = 5, P = 205\n",
    "def count_params3(R: int, C: int, L: int, add_dt: bool = False) -> int:\n",
    "    n = R * C\n",
    "    per_layer = 2*n + R*(C-1) + (R-1)*C + (1 if add_dt else 0)\n",
    "    return per_layer * L\n",
    "\n",
    "# n = 12, L = 5, keep_edges = 20, extras = 6, P = 226\n",
    "def count_params4(n: int, L: int, keep_edges: int, extras: int = 6) -> int:\n",
    "    return 2*L*n + L*keep_edges + extras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f0cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ ansatz factory ------------\n",
    "\n",
    "from src.circuits.ansatz1 import hardware_efficient_ansatz\n",
    "from src.circuits.ansatz2 import ising_structured_ansatz\n",
    "from src.circuits.ansatz3 import eh2d_ansatz\n",
    "from src.circuits.ansatz4 import mi_ansatz\n",
    "## Control # of Params around 100\n",
    "\n",
    "\n",
    "ansatz = ising_structured_ansatz\n",
    "n_qubits= 12\n",
    "mmd_fn = mmdagg_prob\n",
    "R = 3\n",
    "C = 4\n",
    "keep_edges = 20\n",
    "L1 = 10\n",
    "L2 = 5\n",
    "\n",
    "def ansatz_set(ansatz):\n",
    "    if ansatz == hardware_efficient_ansatz:\n",
    "        pc = count_params1(n_bits, L1)\n",
    "        L = L1\n",
    "        id = 1\n",
    "\n",
    "    if ansatz == ising_structured_ansatz:\n",
    "        pc = count_params2(R, C, L2, False)\n",
    "        L = L2\n",
    "        id = 2\n",
    "\n",
    "    if ansatz == eh2d_ansatz:\n",
    "        pc = count_params3(R, C, L2)\n",
    "        L = L2\n",
    "        id = 3\n",
    "\n",
    "    if ansatz == mi_ansatz:\n",
    "        pc = count_params4(n_qubits, L2, keep_edges)\n",
    "        L = L2\n",
    "        id = 4\n",
    "        bit_np = df[bit_cols].values\n",
    "        mi_mat = mutual_information_matrix(bit_np)\n",
    "        triu_i, triu_j = jnp.triu_indices(n_qubits, k=1)\n",
    "        mi_flat   = mi_mat[triu_i, triu_j]\n",
    "        top_idx   = jnp.argsort(-mi_flat)[:keep_edges]\n",
    "        mi_edges  = [(int(triu_i[k]), int(triu_j[k])) for k in top_idx]  # [(i,j),...]\n",
    "        def ansatz_mi(params, wires, *, L=None, **kw):\n",
    "            return mi_ansatz(\n",
    "                params, wires,\n",
    "                mi_edges = mi_edges,\n",
    "                L = L,\n",
    "                **kw\n",
    "            )\n",
    "        ansatz = ansatz_mi\n",
    "    return ansatz, L, pc, id\n",
    "ansatz, L, pc, id = ansatz_set(ansatz)\n",
    "\n",
    "model = QCBM(ansatz, n_bits, L, mmd_fn, target_probs)\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = jax.random.normal(key, shape=(pc,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee8ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------ quick sanity check ------------\n",
    "# key  = jax.random.PRNGKey(0)\n",
    "# params = jax.random.normal(key, (pc,), dtype=jnp.float64)\n",
    "\n",
    "# loss_val = model.loss(params)\n",
    "# print(\"loss =\", loss_val, \"dtype:\", loss_val.dtype, \"device:\", loss_val.device)\n",
    "\n",
    "# grads = jax.grad(model.loss)(params)\n",
    "# print(\"grads shape\", grads.shape, \"dtype:\", grads.dtype, \"device:\", grads.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed1868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ training set up ------------\n",
    "import optax\n",
    "import catalyst\n",
    "\n",
    "# opt = optax.adam(1e-2)\n",
    "\n",
    "lr_sched = optax.exponential_decay(  # 从 1e‑2 → 每 200 步 × 0.9\n",
    "    init_value=1e-2, transition_steps=200, decay_rate=0.9, staircase=True\n",
    ")\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adam(lr_sched, b2=0.999)\n",
    ")\n",
    "\n",
    "\n",
    "def update_step(i, params, opt_state, loss_log):\n",
    "    loss_val, grads = catalyst.value_and_grad(model.loss)(params)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    loss_log = loss_log.at[i].set(loss_val)\n",
    "    grad_norm = optax.global_norm(grads)\n",
    "    catalyst.debug.print(\"Step {i}: loss = {loss}, grad_norm = {g}\", i=i, loss=loss_val, g=grad_norm)\n",
    "\n",
    "    return (params, opt_state, loss_log)\n",
    "\n",
    "@qml.qjit     \n",
    "def optimization(params, n_steps: int = 1000):\n",
    "    opt_state = opt.init(params)\n",
    "    loss_log  = jnp.zeros(n_steps, dtype=params.dtype)\n",
    "    params, opt_state, loss_log = qml.for_loop(\n",
    "        0, n_steps, 1\n",
    "    )(update_step)(params, opt_state, loss_log)\n",
    "    return params, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540ecd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# circuit = model.circuit\n",
    "\n",
    "# @jax.jit\n",
    "# def loss(params):\n",
    "#     qcbm_probs = circuit(params)\n",
    "\n",
    "#     def kl_div(p, q, eps=1e-10):\n",
    "#         p = jnp.clip(p, eps, 1.0)\n",
    "#         q = jnp.clip(q, eps, 1.0)\n",
    "#         p = p / p.sum()\n",
    "#         q = q / q.sum()\n",
    "#         return jnp.sum(p * jnp.log(p / q))\n",
    "\n",
    "#     loss_mmd = mmd_fn(\n",
    "#         target_probs,\n",
    "#         qcbm_probs,\n",
    "#         kernel=\"laplace_gaussian\",\n",
    "#         number_bandwidths=10,\n",
    "#         weights_type=\"uniform\",\n",
    "#         build_details=False,  \n",
    "#         dtype=jnp.float64,\n",
    "#     )\n",
    "#     return loss_mmd * 0.7 + kl_div(target_probs, qcbm_probs) * 0.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3956c35",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from jax import jit, value_and_grad\n",
    "# import optax\n",
    "\n",
    "# # === 超参数 ===\n",
    "# n_steps = 1000\n",
    "# batch_size = 64\n",
    "# noise_std = 0.01\n",
    "\n",
    "# # === 优化器设定 ===\n",
    "# lr_sched = optax.exponential_decay(\n",
    "#     init_value=1e-2, transition_steps=200, decay_rate=0.9, staircase=True\n",
    "# )\n",
    "# optimizer = optax.chain(\n",
    "#     optax.clip_by_global_norm(1.0),\n",
    "#     optax.adam(lr_sched, b2=0.999)\n",
    "# )\n",
    "\n",
    "# # === batch loss 函数（每一步用不同 key）===\n",
    "# def batched_loss_fn(params, step):\n",
    "#     key = jax.random.PRNGKey(step)\n",
    "#     noise = noise_std * jax.random.normal(key, (batch_size, params.shape[0]))\n",
    "#     params_batch = params + noise\n",
    "\n",
    "#     losses = jax.vmap(lambda p: loss(p))(params_batch)\n",
    "#     return jnp.mean(losses)\n",
    "\n",
    "# # === 包装成 JIT + grad ===\n",
    "# @jit\n",
    "# def grad_fn_with_step(params, step):\n",
    "#     return value_and_grad(lambda p: batched_loss_fn(p, step))(params)\n",
    "\n",
    "# # === 单步训练 ===\n",
    "# @jit\n",
    "# def update_step(i, params, opt_state, loss_log):\n",
    "#     loss, grads = grad_fn_with_step(params, i)\n",
    "#     updates, opt_state = optimizer.update(grads, opt_state)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "#     loss_log = loss_log.at[i].set(loss)\n",
    "#     return params, opt_state, loss_log\n",
    "\n",
    "# # === 主训练循环 ===\n",
    "# def make_optimization(n_steps):\n",
    "#     @jit\n",
    "#     def optimization(params):\n",
    "#         opt_state = optimizer.init(params)\n",
    "#         loss_log = jnp.zeros(n_steps, dtype=params.dtype)\n",
    "\n",
    "#         def body(i, val):\n",
    "#             params, opt_state, loss_log = val\n",
    "#             return update_step(i, params, opt_state, loss_log)\n",
    "\n",
    "#         return jax.lax.fori_loop(0, n_steps, body, (params, opt_state, loss_log))\n",
    "\n",
    "#     return optimization\n",
    "\n",
    "# # === 创建并执行训练 ===\n",
    "# optimization = make_optimization(n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 2.1227703898063854, grad_norm = 1.7811951913498185\n",
      "Step 1: loss = 1.979033796085783, grad_norm = 1.5440016006809383\n",
      "Step 2: loss = 1.8565502380918972, grad_norm = 1.3609381344967806\n",
      "Step 3: loss = 1.7506885748103371, grad_norm = 1.2145111212377147\n",
      "Step 4: loss = 1.6582696498328624, grad_norm = 1.0939795831292471\n",
      "Step 5: loss = 1.5769218954175648, grad_norm = 0.993272973357924\n",
      "Step 6: loss = 1.5045233119027128, grad_norm = 0.9090857998676676\n",
      "Step 7: loss = 1.4395832217923097, grad_norm = 0.8360875420097215\n",
      "Step 8: loss = 1.3811703869025727, grad_norm = 0.7684622240196167\n",
      "Step 9: loss = 1.3287643109448601, grad_norm = 0.7058891850439615\n",
      "Step 10: loss = 1.2817679117514598, grad_norm = 0.6488233305261631\n",
      "Step 11: loss = 1.239480044847387, grad_norm = 0.5970893753792966\n",
      "Step 12: loss = 1.20135421694948, grad_norm = 0.5492544551016841\n",
      "Step 13: loss = 1.1668802716202835, grad_norm = 0.5067863853323277\n",
      "Step 14: loss = 1.1356970089499097, grad_norm = 0.46470739421314045\n",
      "Step 15: loss = 1.1078486133069114, grad_norm = 0.42990591775641795\n",
      "Step 16: loss = 1.0816747952509675, grad_norm = 0.40837664836503534\n",
      "Step 17: loss = 1.0575986472993921, grad_norm = 0.3714051756898387\n",
      "Step 18: loss = 1.0362649218013984, grad_norm = 0.33819314955658286\n",
      "Step 19: loss = 1.0174207917714546, grad_norm = 0.30957064423558\n",
      "Step 20: loss = 1.0007615229088827, grad_norm = 0.28478726994830544\n",
      "Step 21: loss = 0.9859948486385248, grad_norm = 0.26341064459014935\n",
      "Step 22: loss = 0.9728467553718734, grad_norm = 0.2449903995275978\n",
      "Step 23: loss = 0.9610777112247806, grad_norm = 0.22907581456431894\n",
      "Step 24: loss = 0.9504924030369826, grad_norm = 0.21548835971578395\n",
      "Step 25: loss = 0.9409141379418062, grad_norm = 0.20416262807930516\n",
      "Step 26: loss = 0.9321578526349957, grad_norm = 0.1946527439136081\n",
      "Step 27: loss = 0.924046515344627, grad_norm = 0.18590033653094998\n",
      "Step 28: loss = 0.9164742005474081, grad_norm = 0.17716900961526033\n",
      "Step 29: loss = 0.9094132117711874, grad_norm = 0.1690358804645117\n",
      "Step 30: loss = 0.9028539753312629, grad_norm = 0.16178514413828576\n",
      "Step 31: loss = 0.8967904115825202, grad_norm = 0.1556114936406939\n",
      "Step 32: loss = 0.8912061163113502, grad_norm = 0.15065090073997728\n",
      "Step 33: loss = 0.8860516210445688, grad_norm = 0.14710006151219274\n",
      "Step 34: loss = 0.8812473083895763, grad_norm = 0.14513581606868217\n",
      "Step 35: loss = 0.8767489101430699, grad_norm = 0.1439999061602587\n",
      "Step 36: loss = 0.872499181027865, grad_norm = 0.1433192016582653\n",
      "Step 37: loss = 0.8684277702042356, grad_norm = 0.14279796563082892\n",
      "Step 38: loss = 0.8644739335229795, grad_norm = 0.1421594072811568\n",
      "Step 39: loss = 0.8605935598539416, grad_norm = 0.1412201549993969\n",
      "Step 40: loss = 0.856759732786733, grad_norm = 0.13990718353620837\n",
      "Step 41: loss = 0.8529597538831006, grad_norm = 0.1382395881586904\n",
      "Step 42: loss = 0.8491905086549358, grad_norm = 0.13629511346645717\n",
      "Step 43: loss = 0.8454538691552593, grad_norm = 0.13417621184652645\n",
      "Step 44: loss = 0.8417531398029459, grad_norm = 0.13198330209489736\n",
      "Step 45: loss = 0.838090782233606, grad_norm = 0.12979680987767642\n",
      "Step 46: loss = 0.8344672386078, grad_norm = 0.1276666536263324\n",
      "Step 47: loss = 0.8308806397707962, grad_norm = 0.12560817607345284\n",
      "Step 48: loss = 0.8273272631013365, grad_norm = 0.12360622439980463\n",
      "Step 49: loss = 0.8238025363503786, grad_norm = 0.12162911270163325\n",
      "Step 50: loss = 0.820302235769035, grad_norm = 0.11964748996954924\n",
      "Step 51: loss = 0.8168235472189488, grad_norm = 0.11764912268961963\n",
      "Step 52: loss = 0.8133657681133236, grad_norm = 0.11564630962446969\n",
      "Step 53: loss = 0.8099304112937095, grad_norm = 0.11367800757610877\n",
      "Step 54: loss = 0.8065204775364877, grad_norm = 0.11180714052982023\n",
      "Step 55: loss = 0.803138910820719, grad_norm = 0.1101139005228022\n",
      "Step 56: loss = 0.7997865013479225, grad_norm = 0.10868778246115553\n",
      "Step 57: loss = 0.7964596514948724, grad_norm = 0.10761831616676411\n",
      "Step 58: loss = 0.7931485309651635, grad_norm = 0.10697530524228091\n",
      "Step 59: loss = 0.7898364551380528, grad_norm = 0.10676485694327226\n",
      "Step 60: loss = 0.7865021189304717, grad_norm = 0.10686423667633277\n",
      "Step 61: loss = 0.7831274642908693, grad_norm = 0.10701282887247039\n",
      "Step 62: loss = 0.7797078903727733, grad_norm = 0.10700806311049813\n",
      "Step 63: loss = 0.7762517396836028, grad_norm = 0.10686843159142027\n",
      "Step 64: loss = 0.7727709159851855, grad_norm = 0.10671998617057055\n",
      "Step 65: loss = 0.7692747693732555, grad_norm = 0.10668081839001839\n",
      "Step 66: loss = 0.7657682266550669, grad_norm = 0.10681855498958098\n",
      "Step 67: loss = 0.762252626013189, grad_norm = 0.10711513465633203\n",
      "Step 68: loss = 0.758729273808231, grad_norm = 0.10746829173275135\n",
      "Step 69: loss = 0.7552028249038084, grad_norm = 0.10776177010293611\n",
      "Step 70: loss = 0.7516807765549486, grad_norm = 0.1079335820863079\n",
      "Step 71: loss = 0.7481705860048333, grad_norm = 0.10797093185841146\n",
      "Step 72: loss = 0.744677928246924, grad_norm = 0.10787322631163831\n",
      "Step 73: loss = 0.7412065740213349, grad_norm = 0.10763407772123837\n",
      "Step 74: loss = 0.7377588809372635, grad_norm = 0.10724132448473594\n",
      "Step 75: loss = 0.7343363740042839, grad_norm = 0.10668206261051809\n",
      "Step 76: loss = 0.7309403283555339, grad_norm = 0.10594748996319445\n",
      "Step 77: loss = 0.7275722979969605, grad_norm = 0.10503655445319732\n",
      "Step 78: loss = 0.7242344645913297, grad_norm = 0.10395795982513979\n",
      "Step 79: loss = 0.7209296963145149, grad_norm = 0.10273017300359559\n",
      "Step 80: loss = 0.7176613112583442, grad_norm = 0.10137955972964781\n",
      "Step 81: loss = 0.7144326516793661, grad_norm = 0.0999373603556395\n",
      "Step 82: loss = 0.7112466259975011, grad_norm = 0.09843649345536408\n",
      "Step 83: loss = 0.7081053500149039, grad_norm = 0.09690897013012068\n",
      "Step 84: loss = 0.7050099488012544, grad_norm = 0.09538421345626587\n",
      "Step 85: loss = 0.701960511451306, grad_norm = 0.09388810193720487\n",
      "Step 86: loss = 0.6989561585367445, grad_norm = 0.09244237900064607\n",
      "Step 87: loss = 0.6959951839219656, grad_norm = 0.09106434228233595\n",
      "Step 88: loss = 0.6930752461763399, grad_norm = 0.08976686149264111\n",
      "Step 89: loss = 0.6901935917909922, grad_norm = 0.0885586005391033\n",
      "Step 90: loss = 0.6873472819931766, grad_norm = 0.08744374806589736\n",
      "Step 91: loss = 0.6845333151610913, grad_norm = 0.08642082182354048\n",
      "Step 92: loss = 0.6817478384230712, grad_norm = 0.08549167712639164\n",
      "Step 93: loss = 0.6789786644976212, grad_norm = 0.08478183908098555\n",
      "Step 94: loss = 0.6761817517098784, grad_norm = 0.08479521385301443\n",
      "Step 95: loss = 0.6733315028467726, grad_norm = 0.08486449120918689\n",
      "Step 96: loss = 0.6704679799660335, grad_norm = 0.0843758729904705\n",
      "Step 97: loss = 0.6676142435321271, grad_norm = 0.08371956321008876\n",
      "Step 98: loss = 0.6647773885503974, grad_norm = 0.08309254321774978\n",
      "Step 99: loss = 0.6619553870821074, grad_norm = 0.08255495470845897\n",
      "Step 100: loss = 0.659138564545564, grad_norm = 0.0821154638135301\n",
      "Step 101: loss = 0.656303796238472, grad_norm = 0.08190698625755344\n",
      "Step 102: loss = 0.6534127932188031, grad_norm = 0.08154654821537154\n",
      "Step 103: loss = 0.6504883351287662, grad_norm = 0.08071431966254206\n",
      "Step 104: loss = 0.6475669646937713, grad_norm = 0.07981675597868124\n",
      "Step 105: loss = 0.6446658375020887, grad_norm = 0.07891450570252866\n",
      "Step 106: loss = 0.6417940500361728, grad_norm = 0.07795543113963152\n",
      "Step 107: loss = 0.6389562781036501, grad_norm = 0.07684590833978006\n",
      "Step 108: loss = 0.6361573810490009, grad_norm = 0.07550509067174399\n",
      "Step 109: loss = 0.6334060090637254, grad_norm = 0.07391021037054583\n",
      "Step 110: loss = 0.6307150799315691, grad_norm = 0.07209982224698788\n",
      "Step 111: loss = 0.6280996346459967, grad_norm = 0.07013733426712372\n",
      "Step 112: loss = 0.6255740475354943, grad_norm = 0.06806896845311834\n",
      "Step 113: loss = 0.6231501888071248, grad_norm = 0.06590787220438968\n",
      "Step 114: loss = 0.6208372318973168, grad_norm = 0.06364415914341046\n",
      "Step 115: loss = 0.6186429596110009, grad_norm = 0.061272004547316725\n",
      "Step 116: loss = 0.6165752548301123, grad_norm = 0.05882075520533389\n",
      "Step 117: loss = 0.61464212102128, grad_norm = 0.056363720046182716\n",
      "Step 118: loss = 0.6128498679298229, grad_norm = 0.05399061970189161\n",
      "Step 119: loss = 0.611200874158956, grad_norm = 0.0517635075933192\n",
      "Step 120: loss = 0.6096927562859733, grad_norm = 0.049692552537938646\n",
      "Step 121: loss = 0.6083194638740814, grad_norm = 0.0477498406723868\n",
      "Step 122: loss = 0.6070732404364219, grad_norm = 0.04590629095764902\n",
      "Step 123: loss = 0.6059459762503966, grad_norm = 0.044158945336251955\n",
      "Step 124: loss = 0.6049293176498227, grad_norm = 0.042525135824526335\n",
      "Step 125: loss = 0.6040141011166255, grad_norm = 0.04100938109754259\n",
      "Step 126: loss = 0.6031902370616802, grad_norm = 0.03957749048825521\n",
      "Step 127: loss = 0.6024475926006155, grad_norm = 0.03817058785771434\n",
      "Step 128: loss = 0.6017772299064499, grad_norm = 0.03675242106070804\n",
      "Step 129: loss = 0.6011717785838804, grad_norm = 0.03534403890908065\n",
      "Step 130: loss = 0.6006244933247391, grad_norm = 0.03400608494253196\n",
      "Step 131: loss = 0.6001279684516594, grad_norm = 0.0327786470903055\n",
      "Step 132: loss = 0.5996740338270355, grad_norm = 0.03164049358275148\n",
      "Step 133: loss = 0.5992551442041604, grad_norm = 0.03054815600890765\n",
      "Step 134: loss = 0.5988655672307068, grad_norm = 0.02950785287002006\n",
      "Step 135: loss = 0.598500885638826, grad_norm = 0.02856802140943634\n",
      "Step 136: loss = 0.5981568125354988, grad_norm = 0.027755287988069366\n",
      "Step 137: loss = 0.5978290279250924, grad_norm = 0.027053190278773894\n",
      "Step 138: loss = 0.5975141095810437, grad_norm = 0.026438957571116573\n",
      "Step 139: loss = 0.5972102833942097, grad_norm = 0.025914687132852048\n",
      "Step 140: loss = 0.5969171731890917, grad_norm = 0.025487271925388754\n",
      "Step 141: loss = 0.5966350625418142, grad_norm = 0.025127734571744928\n",
      "Step 142: loss = 0.5963645612386986, grad_norm = 0.02476806782879418\n",
      "Step 143: loss = 0.5961067935227696, grad_norm = 0.02434377515032904\n",
      "Step 144: loss = 0.5958634934117586, grad_norm = 0.023837750341159505\n",
      "Step 145: loss = 0.5956365141964874, grad_norm = 0.02328514570630052\n",
      "Step 146: loss = 0.5954269439823248, grad_norm = 0.02273993438449225\n",
      "Step 147: loss = 0.595234461400367, grad_norm = 0.02223551219055596\n",
      "Step 148: loss = 0.5950573581581026, grad_norm = 0.02177022968463869\n",
      "Step 149: loss = 0.5948931010089835, grad_norm = 0.021320958803625042\n",
      "Step 150: loss = 0.5947389768375976, grad_norm = 0.020863569799237893\n",
      "Step 151: loss = 0.5945924956150431, grad_norm = 0.02038170783468014\n",
      "Step 152: loss = 0.594451562269487, grad_norm = 0.019864911168870927\n",
      "Step 153: loss = 0.5943145875509004, grad_norm = 0.019308371421642077\n",
      "Step 154: loss = 0.5941805930207044, grad_norm = 0.018719660886830504\n",
      "Step 155: loss = 0.5940491991932279, grad_norm = 0.018124672836019408\n",
      "Step 156: loss = 0.5939204020638815, grad_norm = 0.017562265289811745\n",
      "Step 157: loss = 0.5937942221419816, grad_norm = 0.01706757593883844\n",
      "Step 158: loss = 0.5936704371226786, grad_norm = 0.016656043492790023\n",
      "Step 159: loss = 0.5935485414855918, grad_norm = 0.01632022076682825\n",
      "Step 160: loss = 0.593427894039291, grad_norm = 0.016038965233267734\n",
      "Step 161: loss = 0.5933079073389897, grad_norm = 0.015788290238596184\n",
      "Step 162: loss = 0.5931881794392547, grad_norm = 0.015546364216552823\n",
      "Step 163: loss = 0.5930685709427308, grad_norm = 0.015295059048517691\n",
      "Step 164: loss = 0.5929492590194233, grad_norm = 0.015023527713816133\n",
      "Step 165: loss = 0.5928307495296343, grad_norm = 0.014733490958571023\n",
      "Step 166: loss = 0.5927137922454625, grad_norm = 0.014440468444748762\n",
      "Step 167: loss = 0.592599193304852, grad_norm = 0.014166354444593998\n",
      "Step 168: loss = 0.5924876062477811, grad_norm = 0.013927616543629256\n",
      "Step 169: loss = 0.5923794050347276, grad_norm = 0.013727174259629498\n",
      "Step 170: loss = 0.5922746848458615, grad_norm = 0.013554931159425427\n",
      "Step 171: loss = 0.5921733527362111, grad_norm = 0.013394473302054353\n",
      "Step 172: loss = 0.592075234988469, grad_norm = 0.013229733478119964\n",
      "Step 173: loss = 0.5919801545782319, grad_norm = 0.013048179233871197\n",
      "Step 174: loss = 0.591887975529983, grad_norm = 0.012841612302258838\n",
      "Step 175: loss = 0.5917986270506799, grad_norm = 0.012606937327467036\n",
      "Step 176: loss = 0.5917121086215267, grad_norm = 0.012347244108300306\n",
      "Step 177: loss = 0.5916284674268762, grad_norm = 0.012071633731947033\n",
      "Step 178: loss = 0.5915477502280552, grad_norm = 0.011792721934113286\n",
      "Step 179: loss = 0.5914699513532724, grad_norm = 0.011522839445405683\n",
      "Step 180: loss = 0.5913949834050333, grad_norm = 0.011271156419936933\n",
      "Step 181: loss = 0.591322680961391, grad_norm = 0.011042996844299272\n",
      "Step 182: loss = 0.5912528260015667, grad_norm = 0.010840630418272774\n",
      "Step 183: loss = 0.5911851762027196, grad_norm = 0.010663996855784483\n",
      "Step 184: loss = 0.591119486021744, grad_norm = 0.010510780367677839\n",
      "Step 185: loss = 0.591055522091094, grad_norm = 0.010376548453698994\n",
      "Step 186: loss = 0.5909930768093532, grad_norm = 0.010255696373207857\n",
      "Step 187: loss = 0.5909319786151538, grad_norm = 0.010142867108644326\n",
      "Step 188: loss = 0.5908720943288533, grad_norm = 0.010033829210479312\n",
      "Step 189: loss = 0.590813323171789, grad_norm = 0.009925320389212226\n",
      "Step 190: loss = 0.5907555885036865, grad_norm = 0.00981439693852184\n",
      "Step 191: loss = 0.5906988341832119, grad_norm = 0.009698175490713133\n",
      "Step 192: loss = 0.5906430268066134, grad_norm = 0.009574247248947131\n",
      "Step 193: loss = 0.5905881591389234, grad_norm = 0.009441285307695573\n",
      "Step 194: loss = 0.5905342495147384, grad_norm = 0.009299268862725137\n",
      "Step 195: loss = 0.5904813361073031, grad_norm = 0.009149266442619668\n",
      "Step 196: loss = 0.5904294685997954, grad_norm = 0.008993161956415011\n",
      "Step 197: loss = 0.5903786997060699, grad_norm = 0.008833603708536192\n",
      "Step 198: loss = 0.5903290768017531, grad_norm = 0.008674026403961104\n",
      "Step 199: loss = 0.5902806331550708, grad_norm = 0.008518390413841377\n",
      "Step 200: loss = 0.5902333798417685, grad_norm = 0.008370526872008342\n",
      "Step 201: loss = 0.5901918678240616, grad_norm = 0.00824644668520228\n",
      "Step 202: loss = 0.5901512929670338, grad_norm = 0.008132031594240526\n",
      "Step 203: loss = 0.5901116058293353, grad_norm = 0.008027557749446761\n",
      "Step 204: loss = 0.5900727485662773, grad_norm = 0.007932369151984519\n",
      "Step 205: loss = 0.5900346600051068, grad_norm = 0.007845077163895599\n",
      "Step 206: loss = 0.5899972799107968, grad_norm = 0.0077638316844213994\n",
      "Step 207: loss = 0.5899605523080761, grad_norm = 0.007686680351474328\n",
      "Step 208: loss = 0.589924427771016, grad_norm = 0.00761196678480262\n",
      "Step 209: loss = 0.5898888643784487, grad_norm = 0.007538629863285014\n",
      "Step 210: loss = 0.5898538271586814, grad_norm = 0.007466271926707128\n",
      "Step 211: loss = 0.589819286409821, grad_norm = 0.007394983486535299\n",
      "Step 212: loss = 0.5897852158016823, grad_norm = 0.0073250473715239075\n",
      "Step 213: loss = 0.5897515911432635, grad_norm = 0.007256686406778201\n",
      "Step 214: loss = 0.5897183901464039, grad_norm = 0.007189948872655322\n",
      "Step 215: loss = 0.5896855928974373, grad_norm = 0.007124722688650051\n",
      "Step 216: loss = 0.5896531824960808, grad_norm = 0.007060816891746881\n",
      "Step 217: loss = 0.5896211454618231, grad_norm = 0.006998061639757093\n",
      "Step 218: loss = 0.5895894717553456, grad_norm = 0.00693640702789151\n",
      "Step 219: loss = 0.5895581543797208, grad_norm = 0.006876002939679117\n",
      "Step 220: loss = 0.5895271885393355, grad_norm = 0.0068172246204108245\n",
      "Step 221: loss = 0.5894965704107983, grad_norm = 0.0067606104997335525\n",
      "Step 222: loss = 0.5894662957706266, grad_norm = 0.0067067174538794055\n",
      "Step 223: loss = 0.5894363588846472, grad_norm = 0.006655947736872777\n",
      "Step 224: loss = 0.5894067520180504, grad_norm = 0.0066084194846775055\n",
      "Step 225: loss = 0.5893774656665597, grad_norm = 0.006563925525910267\n",
      "Step 226: loss = 0.5893484893145143, grad_norm = 0.006521978847244513\n",
      "Step 227: loss = 0.5893198123830686, grad_norm = 0.006481912850213325\n",
      "Step 228: loss = 0.5892914250797362, grad_norm = 0.006443001939435947\n",
      "Step 229: loss = 0.5892633189950492, grad_norm = 0.006404577753660681\n",
      "Step 230: loss = 0.5892354873980494, grad_norm = 0.006366121242848019\n",
      "Step 231: loss = 0.5892079252373914, grad_norm = 0.006327311205270335\n",
      "Step 232: loss = 0.5891806289019964, grad_norm = 0.0062880178715418065\n",
      "Step 233: loss = 0.5891535958568764, grad_norm = 0.006248249692338454\n",
      "Step 234: loss = 0.5891268243069354, grad_norm = 0.006208080854043891\n",
      "Step 235: loss = 0.5891003130059621, grad_norm = 0.006167590819233911\n",
      "Step 236: loss = 0.5890740612291889, grad_norm = 0.006126833040858426\n",
      "Step 237: loss = 0.5890480688336377, grad_norm = 0.006085830742452099\n",
      "Step 238: loss = 0.5890223363036997, grad_norm = 0.006044587678657429\n",
      "Step 239: loss = 0.588996864721354, grad_norm = 0.006003103797674535\n",
      "Step 240: loss = 0.5889716556615199, grad_norm = 0.0059613913798502284\n",
      "Step 241: loss = 0.5889467110462799, grad_norm = 0.005919488557904024\n",
      "Step 242: loss = 0.588922032992268, grad_norm = 0.0058774647750116805\n",
      "Step 243: loss = 0.588897623676442, grad_norm = 0.00583541291180112\n",
      "Step 244: loss = 0.5888734852428696, grad_norm = 0.0057934284891351185\n",
      "Step 245: loss = 0.5888496197691209, grad_norm = 0.005751583569680122\n",
      "Step 246: loss = 0.5888260292933524, grad_norm = 0.005709905613301646\n",
      "Step 247: loss = 0.5888027158766543, grad_norm = 0.005668368007645517\n",
      "Step 248: loss = 0.588779681660528, grad_norm = 0.005626893213999814\n",
      "Step 249: loss = 0.5887569288891624, grad_norm = 0.005585365931594321\n",
      "Step 250: loss = 0.588734459892577, grad_norm = 0.005543653022322066\n",
      "Step 251: loss = 0.5887122770474165, grad_norm = 0.005501626711741779\n",
      "Step 252: loss = 0.5886903827348309, grad_norm = 0.005459186322719567\n",
      "Step 253: loss = 0.5886687793044705, grad_norm = 0.005416272966309542\n",
      "Step 254: loss = 0.588647469043652, grad_norm = 0.005372873428267386\n",
      "Step 255: loss = 0.5886264541480962, grad_norm = 0.0053290137635246156\n",
      "Step 256: loss = 0.5886057366916226, grad_norm = 0.005284747059811201\n",
      "Step 257: loss = 0.5885853185909652, grad_norm = 0.0052401408890077215\n",
      "Step 258: loss = 0.5885652015601145, grad_norm = 0.005195268186474765\n",
      "Step 259: loss = 0.5885453870503612, grad_norm = 0.005150202788525779\n",
      "Step 260: loss = 0.5885258761793559, grad_norm = 0.00510501930305142\n",
      "Step 261: loss = 0.5885066696594201, grad_norm = 0.005059796290205906\n",
      "Step 262: loss = 0.5884877677361909, grad_norm = 0.005014620975213598\n",
      "Step 263: loss = 0.5884691701429589, grad_norm = 0.004969592839273369\n",
      "Step 264: loss = 0.5884508760691632, grad_norm = 0.004924823418463041\n",
      "Step 265: loss = 0.5884328841386998, grad_norm = 0.004880431165393001\n",
      "Step 266: loss = 0.5884151923949115, grad_norm = 0.004836532606878647\n",
      "Step 267: loss = 0.5883977982914766, grad_norm = 0.0047932327325349204\n",
      "Step 268: loss = 0.5883806986891584, grad_norm = 0.004750617632491789\n",
      "Step 269: loss = 0.5883638898587026, grad_norm = 0.004708751208735111\n",
      "Step 270: loss = 0.5883473674912008, grad_norm = 0.004667676298649474\n",
      "Step 271: loss = 0.588331126719012, grad_norm = 0.004627419389974474\n",
      "Step 272: loss = 0.5883151621502524, grad_norm = 0.004587997296886681\n",
      "Step 273: loss = 0.5882994679179353, grad_norm = 0.004549423667216872\n",
      "Step 274: loss = 0.5882840377414386, grad_norm = 0.004511713264782745\n",
      "Step 275: loss = 0.588268864996309, grad_norm = 0.004474882866313138\n",
      "Step 276: loss = 0.5882539427884911, grad_norm = 0.00443894904530861\n"
     ]
    }
   ],
   "source": [
    "# ------------ training steps ------------\n",
    "\n",
    "trained_params, loss_hist = optimization(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57ae05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee1565",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjax\u001b[49m.block_until_ready(trained_params)      \u001b[38;5;66;03m# wait until training ends\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ───── save ─────\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'jax' is not defined"
     ]
    }
   ],
   "source": [
    "jax.block_until_ready(trained_params)      # wait until training ends\n",
    "\n",
    "# ───── save ─────\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "run_id = f\"result{id}\"\n",
    "\n",
    "\n",
    "out_dir = Path(\"../../data/results/Qubits12\")/run_id\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(out_dir / f\"params.npy\",  jax.device_get(trained_params))\n",
    "np.save(out_dir / f\"loss.npy\", jax.device_get(loss_hist))\n",
    "\n",
    "print(f\"model params have saved to: {out_dir / 'params.npy'}\")\n",
    "print(f\"Loss record has saved to: {out_dir / 'loss.npy'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba039fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after reload: 0.017519396353931176\n",
      "KL   : 5.8318e-02\n",
      "KL (target>0 only): 0.023321320713117023\n",
      "L1   : 4.7178e-04\n",
      "MMD  : 3.4205e-05\n"
     ]
    }
   ],
   "source": [
    "# ───── 2. load the model ─────\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "out_dir = Path(f\"../../data/results/result{id}\")\n",
    "loaded_params = jnp.asarray(np.load(out_dir/\"params.npy\"), dtype=jnp.float64)\n",
    "\n",
    "model = QCBM(ansatz, n_bits, L, mmdagg_prob, target_probs)  # 跟训练时相同\n",
    "loss_loaded = loss(loaded_params).block_until_ready()\n",
    "print(\"Loss after reload:\", float(loss_loaded))\n",
    "\n",
    "probs_trained = model.circuit(loaded_params).block_until_ready()\n",
    "kl   = scipy.stats.entropy(target_probs, probs_trained)\n",
    "l1   = jnp.mean(jnp.abs(target_probs - probs_trained))\n",
    "gpu = jax.devices(\"gpu\")[0]\n",
    "mmd, _ = mmdagg_prob(jax.device_put(target_probs, gpu), \n",
    "                     jax.device_put(probs_trained, gpu),\n",
    "                     kernel=\"laplace_gaussian\", number_bandwidths=10)\n",
    "mask = target_probs > 0          # only check the non-zero part\n",
    "kl_nonzero = scipy.stats.entropy(\n",
    "    target_probs[mask],\n",
    "    probs_trained[mask] / probs_trained[mask].sum()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"KL   : {kl:.4e}\")\n",
    "print(\"KL (target>0 only):\", kl_nonzero)\n",
    "print(f\"L1   : {l1:.4e}\")\n",
    "print(f\"MMD  : {mmd:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041dfe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 losses: [1.1846474  1.13500184 1.08817893 1.04396617 1.00213912]\n",
      "Final   loss : 0.0175215416003253\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params_loaded   = loaded_params\n",
    "loss_hist_loaded = np.load(f\"{out_dir}/loss.npy\")\n",
    "\n",
    "print(\"First 5 losses:\", loss_hist_loaded[:5])\n",
    "print(\"Final   loss :\", loss_hist_loaded[-1])\n",
    "\n",
    "plt.semilogy(loss_hist_loaded)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss (log‑scale)\")\n",
    "plt.title(\"Training curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e54516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST  :  KL = 1.4397e+00   L1 = 5.1414e-03   MMD = 2.4332e-02\n",
      "UNSEEN:  KL = 1.5082e+00   L1 = 5.3655e-03   MMD = 2.3912e-02\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR   = \"../../data\"\n",
    "def csv_to_probs(path: str, n_bits=8, dtype=jnp.float64):\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    bitstrings = df[\"bitstring\"].str.strip().str.zfill(n_bits)\n",
    "    counts = bitstrings.value_counts().sort_index()\n",
    "    all_bits = [\"\".join(seq) for seq in product(\"01\", repeat=n_bits)]\n",
    "    probs = pd.Series(0.0, index=all_bits, dtype=float)\n",
    "    probs.update(counts / counts.sum())\n",
    "    return jnp.asarray(probs.values, dtype=dtype)\n",
    "\n",
    "def kl_div(p, q, eps=1e-10):\n",
    "    p = jnp.clip(p, eps, 1.0)\n",
    "    q = jnp.clip(q, eps, 1.0)\n",
    "    p = p / p.sum();  q = q / q.sum()\n",
    "    return jnp.sum(p * jnp.log(p / q))\n",
    "\n",
    "# ---------- load trained params ----------\n",
    "params_loaded = params_loaded\n",
    "\n",
    "# ---------- build QCBM ----------\n",
    "model = QCBM(\n",
    "    ansatz       = ansatz,\n",
    "    n_qubits     = 8,\n",
    "    L            = 4,\n",
    "    mmd_fn       = mmdagg_prob,\n",
    "    target_probs = jnp.zeros(256),   # 占位即可\n",
    "    dtype        = jnp.float64,\n",
    ")\n",
    "\n",
    "# ---------- K1, L1, MMD ----------\n",
    "@jax.jit\n",
    "def three_metrics(target_probs, params):\n",
    "    probs = model.circuit(params)\n",
    "    kl    = kl_div(target_probs, probs)\n",
    "    l1    = jnp.mean(jnp.abs(target_probs - probs))\n",
    "    mmd   = mmdagg_prob(\n",
    "        target_probs, probs,\n",
    "        kernel=\"laplace_gaussian\", number_bandwidths=10,\n",
    "        build_details=False, dtype=jnp.float64\n",
    "    )\n",
    "    return kl, l1, mmd\n",
    "\n",
    "# ---------- calculate & print ----------\n",
    "splits = {\n",
    "    \"TEST\"  : csv_to_probs(\"../../data_2d/test.csv\"),\n",
    "    \"UNSEEN\": csv_to_probs(\"../../data_2d/unseen.csv\"),\n",
    "}\n",
    "\n",
    "for name, tgt in splits.items():\n",
    "    kl, l1, mmd = three_metrics(tgt, params_loaded)\n",
    "    kl, l1, mmd = map(lambda x: float(x.block_until_ready()), (kl, l1, mmd))\n",
    "    print(f\"{name:6s}:  KL = {kl:.4e}   L1 = {l1:.4e}   MMD = {mmd:.4e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
